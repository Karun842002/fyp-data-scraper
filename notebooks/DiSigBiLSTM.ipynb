{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-25T18:39:27.603811Z","iopub.status.busy":"2023-04-25T18:39:27.602597Z","iopub.status.idle":"2023-04-25T18:39:41.823540Z","shell.execute_reply":"2023-04-25T18:39:41.822225Z","shell.execute_reply.started":"2023-04-25T18:39:27.603765Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nltk==3.8.1\n","  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (2021.11.10)\n","Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (1.2.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (4.64.1)\n","Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (8.1.3)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.27.4)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.13.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.14.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.13.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.4.0)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=b504ac372a76e36d217424b4798052c9ceeb426f9b8e7e91c707b80b98bdb2e3\n","  Stored in directory: /root/.cache/pip/wheels/83/71/2b/40d17d21937fed496fb99145227eca8f20b4891240ff60c86f\n","Successfully built sentence-transformers\n","Installing collected packages: nltk, sentence-transformers\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.4\n","    Uninstalling nltk-3.2.4:\n","      Successfully uninstalled nltk-3.2.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nltk-3.8.1 sentence-transformers-2.2.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install nltk==3.8.1 sentence-transformers\n","# !pip install pytorch2tikz"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T18:40:09.189965Z","iopub.status.busy":"2023-04-25T18:40:09.189004Z","iopub.status.idle":"2023-04-25T18:40:09.200145Z","shell.execute_reply":"2023-04-25T18:40:09.199078Z","shell.execute_reply.started":"2023-04-25T18:40:09.189913Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, GRU, Dense\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.layers import BatchNormalization\n","from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n","from tensorflow.keras.preprocessing import sequence, text\n","from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Input, Embedding, Dropout, Conv1D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.losses import BinaryCrossentropy\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T18:40:09.453426Z","iopub.status.busy":"2023-04-25T18:40:09.452490Z","iopub.status.idle":"2023-04-25T18:40:09.506236Z","shell.execute_reply":"2023-04-25T18:40:09.505238Z","shell.execute_reply.started":"2023-04-25T18:40:09.453373Z"},"trusted":true},"outputs":[],"source":["\n","df = pd.read_csv(r'../scraper/datasets/finalDataset/latest3.csv')\n","\n","le = LabelEncoder()\n","cdf = df.copy()\n","cdf['truth_value'] = le.fit_transform(cdf['truth_value'])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T18:40:10.102393Z","iopub.status.busy":"2023-04-25T18:40:10.101673Z","iopub.status.idle":"2023-04-25T18:40:10.116486Z","shell.execute_reply":"2023-04-25T18:40:10.115224Z","shell.execute_reply.started":"2023-04-25T18:40:10.102354Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["nan_mask = cdf['simple_sentence'].isna()\n","nan_indices = cdf[nan_mask].index\n","\n","# Replace NaN values in 'B' with values from column 'A'\n","cdf.loc[nan_indices, 'simple_sentence'] = df.loc[nan_indices, 'claim']\n","\n","cdf['simple_sentence'].isna().sum()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T18:40:10.983015Z","iopub.status.busy":"2023-04-25T18:40:10.982101Z","iopub.status.idle":"2023-04-25T18:40:12.162461Z","shell.execute_reply":"2023-04-25T18:40:12.161450Z","shell.execute_reply.started":"2023-04-25T18:40:10.982973Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n","  \n","/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>claim</th>\n","      <th>simple_sentence</th>\n","      <th>truth_value</th>\n","      <th>source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>ukraine theft homicide levels rose due power o...</td>\n","      <td>ukraine theft homicide levels rose due power o...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>ukrainians beat two berlin residents speaking ...</td>\n","      <td>ukrainians beat two berlin residents speaking ...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>quote paul goebbels banderites</td>\n","      <td>quote paul goebbels banderites</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>culture good neighborliness course ukrainian s...</td>\n","      <td>culture good neighborliness course ukrainian s...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>us research ukraine led increase incidence tic...</td>\n","      <td>us research ukraine led increase incidence tic...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>chile law rights mutants genetically modified ...</td>\n","      <td>chile law rights mutants approved ostap stakhi...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>covid incidence rate became zero late may</td>\n","      <td>covid incidence rate became zero late may</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>risk death among children vaccinated covid tim...</td>\n","      <td>risk death among children vaccinated covid tim...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>russias army destroyed</td>\n","      <td>russias army destroyed</td>\n","      <td>0</td>\n","      <td>politifact</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>war ukraine</td>\n","      <td>war ukraine</td>\n","      <td>0</td>\n","      <td>politifact</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                              claim  \\\n","0           0  ukraine theft homicide levels rose due power o...   \n","1           1  ukrainians beat two berlin residents speaking ...   \n","2           2                     quote paul goebbels banderites   \n","3           3  culture good neighborliness course ukrainian s...   \n","4           4  us research ukraine led increase incidence tic...   \n","5           5  chile law rights mutants genetically modified ...   \n","6           6          covid incidence rate became zero late may   \n","7           7  risk death among children vaccinated covid tim...   \n","8           8                             russias army destroyed   \n","9           9                                        war ukraine   \n","\n","                                     simple_sentence  truth_value       source  \n","0  ukraine theft homicide levels rose due power o...            0  vox-ukraine  \n","1  ukrainians beat two berlin residents speaking ...            0  vox-ukraine  \n","2                     quote paul goebbels banderites            0  vox-ukraine  \n","3  culture good neighborliness course ukrainian s...            0  vox-ukraine  \n","4  us research ukraine led increase incidence tic...            0  vox-ukraine  \n","5  chile law rights mutants approved ostap stakhi...            0  vox-ukraine  \n","6          covid incidence rate became zero late may            0  vox-ukraine  \n","7  risk death among children vaccinated covid tim...            0  vox-ukraine  \n","8                             russias army destroyed            0   politifact  \n","9                                        war ukraine            0   politifact  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","\n","stemmer = SnowballStemmer(\"english\")\n","def stemm_text(text):\n","    return ' '.join([stemmer.stem(w) for w in text.split(' ')])\n","\n","w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n","lemmatizer = nltk.stem.WordNetLemmatizer()\n","\n","def lemmatize_text(text):\n","    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])\n","\n","T = cdf['claim'].str.split(' \\n\\n---\\n\\n').str[0]\n","T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n","stop = stopwords.words('english')\n","T = T.apply(lambda x: ' '.join([y for y in x.split() if not y.isdigit()]))\n","T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n","cdf['claim'] = T\n","\n","T = cdf['simple_sentence'].str.split(' \\n\\n---\\n\\n').str[0]\n","T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n","stop = stopwords.words('english')\n","T = T.apply(lambda x: ' '.join([y for y in x.split() if not y.isdigit()]))\n","T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n","cdf['simple_sentence'] = T\n","cdf.head(10)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T17:45:29.459441Z","iopub.status.busy":"2023-04-25T17:45:29.458986Z","iopub.status.idle":"2023-04-25T17:45:29.489981Z","shell.execute_reply":"2023-04-25T17:45:29.489046Z","shell.execute_reply.started":"2023-04-25T17:45:29.459401Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('wordnet')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T18:40:18.397081Z","iopub.status.busy":"2023-04-25T18:40:18.396594Z","iopub.status.idle":"2023-04-25T18:40:21.533892Z","shell.execute_reply":"2023-04-25T18:40:21.532686Z","shell.execute_reply.started":"2023-04-25T18:40:18.397038Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  import sys\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>claim</th>\n","      <th>simple_sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>kyiv developed plan genocide population donbas</td>\n","      <td>kyiv developed plan genocide population donbas</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>president joe biden said idea going send tank ...</td>\n","      <td>president joe biden said idea going send tank ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>asymptomatic carrier coronavirus cannot infect...</td>\n","      <td>asymptomatic carrier coronavirus cannot infect...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>war ukraine sky kyiv come drone attack u poise...</td>\n","      <td>war ukraine sky kyiv come drone attack u poise...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>u troop ground ukraine</td>\n","      <td>u troop ground ukraine</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               claim  \\\n","0     kyiv developed plan genocide population donbas   \n","1  president joe biden said idea going send tank ...   \n","2  asymptomatic carrier coronavirus cannot infect...   \n","3  war ukraine sky kyiv come drone attack u poise...   \n","4                             u troop ground ukraine   \n","\n","                                     simple_sentence  \n","0     kyiv developed plan genocide population donbas  \n","1  president joe biden said idea going send tank ...  \n","2  asymptomatic carrier coronavirus cannot infect...  \n","3  war ukraine sky kyiv come drone attack u poise...  \n","4                             u troop ground ukraine  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["cdf = cdf.sample(frac=1).reset_index(drop=True)\n","X = cdf[['claim', 'simple_sentence']]\n","y = cdf['truth_value']\n","\n","# X = X.apply(lambda w: lemmatize_text(w))\n","X['claim'] = X['claim'].apply(lambda w: lemmatize_text(w))\n","X['simple_sentence'] = X['simple_sentence'].apply(lambda w: lemmatize_text(w))\n","X.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T17:41:58.752347Z","iopub.status.busy":"2023-04-25T17:41:58.751492Z","iopub.status.idle":"2023-04-25T17:41:59.797971Z","shell.execute_reply":"2023-04-25T17:41:59.796589Z","shell.execute_reply.started":"2023-04-25T17:41:58.752307Z"},"trusted":true},"outputs":[],"source":["!mkdir /root/nltk_data"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T17:41:59.804101Z","iopub.status.busy":"2023-04-25T17:41:59.803125Z","iopub.status.idle":"2023-04-25T17:42:00.131737Z","shell.execute_reply":"2023-04-25T17:42:00.130714Z","shell.execute_reply.started":"2023-04-25T17:41:59.804051Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to ./wordnet...\n"]},{"data":{"text/plain":["True"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('wordnet', download_dir='./wordnet')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T17:42:00.133643Z","iopub.status.busy":"2023-04-25T17:42:00.133272Z","iopub.status.idle":"2023-04-25T17:42:01.402023Z","shell.execute_reply":"2023-04-25T17:42:01.400772Z","shell.execute_reply.started":"2023-04-25T17:42:00.133605Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  ./wordnet/corpora/wordnet.zip\n","   creating: /root/nltk_data/corpora/wordnet/\n","  inflating: /root/nltk_data/corpora/wordnet/lexnames  \n","  inflating: /root/nltk_data/corpora/wordnet/data.verb  \n","  inflating: /root/nltk_data/corpora/wordnet/index.adv  \n","  inflating: /root/nltk_data/corpora/wordnet/adv.exc  \n","  inflating: /root/nltk_data/corpora/wordnet/index.verb  \n","  inflating: /root/nltk_data/corpora/wordnet/cntlist.rev  \n","  inflating: /root/nltk_data/corpora/wordnet/data.adj  \n","  inflating: /root/nltk_data/corpora/wordnet/index.adj  \n","  inflating: /root/nltk_data/corpora/wordnet/LICENSE  \n","  inflating: /root/nltk_data/corpora/wordnet/citation.bib  \n","  inflating: /root/nltk_data/corpora/wordnet/noun.exc  \n","  inflating: /root/nltk_data/corpora/wordnet/verb.exc  \n","  inflating: /root/nltk_data/corpora/wordnet/README  \n","  inflating: /root/nltk_data/corpora/wordnet/index.sense  \n","  inflating: /root/nltk_data/corpora/wordnet/data.noun  \n","  inflating: /root/nltk_data/corpora/wordnet/data.adv  \n","  inflating: /root/nltk_data/corpora/wordnet/index.noun  \n","  inflating: /root/nltk_data/corpora/wordnet/adj.exc  \n"]}],"source":["!unzip -o ./wordnet/corpora/wordnet.zip -d /root/nltk_data/corpora/ "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T18:44:47.424662Z","iopub.status.busy":"2023-04-25T18:44:47.423914Z","iopub.status.idle":"2023-04-25T18:44:47.429993Z","shell.execute_reply":"2023-04-25T18:44:47.428850Z","shell.execute_reply.started":"2023-04-25T18:44:47.424614Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T18:44:53.443961Z","iopub.status.busy":"2023-04-25T18:44:53.443462Z","iopub.status.idle":"2023-04-25T18:44:53.454642Z","shell.execute_reply":"2023-04-25T18:44:53.453464Z","shell.execute_reply.started":"2023-04-25T18:44:53.443914Z"},"trusted":true},"outputs":[],"source":["\n","# Define the model architecture\n","class BiLSTM(nn.Module):\n","    def __init__(self, num_words, embed_size, hidden_size, fc_out_size, output_size, dropout_rate):\n","        super(BiLSTM, self).__init__()\n","        self.embedding = nn.Embedding(num_words, embed_size)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.bilstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n","        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n","#         self.fc = nn.Linear(hidden_size * 2, output_size)\n","        self.fc = nn.Linear(hidden_size * 2, fc_out_size)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.bilstm1(x)\n","        x = self.dropout(x)\n","        x, _ = self.bilstm2(x)\n","        x = self.dropout(x)\n","        x = self.fc(x[:, -1, :])\n","        return x\n","    \n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T18:44:54.472728Z","iopub.status.busy":"2023-04-25T18:44:54.472353Z","iopub.status.idle":"2023-04-25T18:44:54.481242Z","shell.execute_reply":"2023-04-25T18:44:54.479964Z","shell.execute_reply.started":"2023-04-25T18:44:54.472695Z"},"trusted":true},"outputs":[],"source":["\n","# Define the model architecture\n","class BiLSTM2(nn.Module):\n","    def __init__(self, num_words, embed_size, hidden_size, fc_out_size, output_size, dropout_rate):\n","        super(BiLSTM2, self).__init__()\n","        self.embedding = nn.Embedding(num_words, embed_size)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.bilstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n","        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n","#         self.fc = nn.Linear(hidden_size * 2, output_size)\n","        self.fc = nn.Linear(hidden_size * 2, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.bilstm1(x)\n","        x = self.dropout(x)\n","        x, _ = self.bilstm2(x)\n","        x = self.dropout(x)\n","        x = self.fc(x[:, -1, :])\n","        return self.sigmoid(x)\n","    \n"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T19:09:46.429381Z","iopub.status.busy":"2023-04-25T19:09:46.428907Z","iopub.status.idle":"2023-04-25T19:11:37.819467Z","shell.execute_reply":"2023-04-25T19:11:37.818301Z","shell.execute_reply.started":"2023-04-25T19:09:46.429345Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n"," Fold: 0\n","Model 1, Total Epochs: 25, Train Accuracy: 0.9364437532400207 Test Accuracy: 0.8648424543946932\n","Model 2, Total Epochs: 25, Train Accuracy: 0.9318818040435459 Test Accuracy: 0.8756218905472637\n","Model 1 and 2 results on train set:\n","Model 1 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9325    0.9478    0.9401      5075\n","         1.0     0.9409    0.9239    0.9323      4570\n","\n","    accuracy                         0.9364      9645\n","   macro avg     0.9367    0.9358    0.9362      9645\n","weighted avg     0.9365    0.9364    0.9364      9645\n","\n","[[4810  265]\n"," [ 348 4222]]\n","Model 2 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9293    0.9423    0.9357      5075\n","         1.0     0.9349    0.9204    0.9276      4570\n","\n","    accuracy                         0.9319      9645\n","   macro avg     0.9321    0.9313    0.9316      9645\n","weighted avg     0.9319    0.9319    0.9319      9645\n","\n","[[4782  293]\n"," [ 364 4206]]\n","Model 1 and 2 results on test set:\n","Model 1 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8637    0.8762    0.8699      1244\n","         1.0     0.8661    0.8527    0.8594      1168\n","\n","    accuracy                         0.8648      2412\n","   macro avg     0.8649    0.8645    0.8646      2412\n","weighted avg     0.8649    0.8648    0.8648      2412\n","\n","[[1090  154]\n"," [ 172  996]]\n","Model 2 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8782    0.8810    0.8796      1244\n","         1.0     0.8729    0.8699    0.8714      1168\n","\n","    accuracy                         0.8756      2412\n","   macro avg     0.8755    0.8754    0.8755      2412\n","weighted avg     0.8756    0.8756    0.8756      2412\n","\n","[[1096  148]\n"," [ 152 1016]]\n","SVM results on train set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9134    0.9764    0.9438      5075\n","         1.0     0.9716    0.8972    0.9329      4570\n","\n","    accuracy                         0.9388      9645\n","   macro avg     0.9425    0.9368    0.9383      9645\n","weighted avg     0.9409    0.9388    0.9386      9645\n","\n","[[4955  120]\n"," [ 470 4100]]\n","SVM results on test set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8504    0.9228    0.8851      1244\n","         1.0     0.9096    0.8271    0.8664      1168\n","\n","    accuracy                         0.8765      2412\n","   macro avg     0.8800    0.8749    0.8757      2412\n","weighted avg     0.8791    0.8765    0.8760      2412\n","\n","[[1148   96]\n"," [ 202  966]]\n","\n","\n"," Fold: 1\n","Model 1, Total Epochs: 25, Train Accuracy: 0.934888543286677 Test Accuracy: 0.8685737976782753\n","Model 2, Total Epochs: 25, Train Accuracy: 0.9358216692586833 Test Accuracy: 0.8739635157545605\n","Model 1 and 2 results on train set:\n","Model 1 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9390    0.9368    0.9379      5065\n","         1.0     0.9303    0.9328    0.9315      4580\n","\n","    accuracy                         0.9349      9645\n","   macro avg     0.9347    0.9348    0.9347      9645\n","weighted avg     0.9349    0.9349    0.9349      9645\n","\n","[[4745  320]\n"," [ 308 4272]]\n","Model 2 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9213    0.9597    0.9401      5065\n","         1.0     0.9533    0.9094    0.9308      4580\n","\n","    accuracy                         0.9358      9645\n","   macro avg     0.9373    0.9346    0.9355      9645\n","weighted avg     0.9365    0.9358    0.9357      9645\n","\n","[[4861  204]\n"," [ 415 4165]]\n","Model 1 and 2 results on test set:\n","Model 1 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8794    0.8660    0.8726      1254\n","         1.0     0.8573    0.8713    0.8642      1158\n","\n","    accuracy                         0.8686      2412\n","   macro avg     0.8683    0.8687    0.8684      2412\n","weighted avg     0.8687    0.8686    0.8686      2412\n","\n","[[1086  168]\n"," [ 149 1009]]\n","Model 2 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8620    0.9019    0.8815      1254\n","         1.0     0.8882    0.8437    0.8654      1158\n","\n","    accuracy                         0.8740      2412\n","   macro avg     0.8751    0.8728    0.8734      2412\n","weighted avg     0.8746    0.8740    0.8738      2412\n","\n","[[1131  123]\n"," [ 181  977]]\n","SVM results on train set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9184    0.9730    0.9449      5065\n","         1.0     0.9680    0.9044    0.9351      4580\n","\n","    accuracy                         0.9404      9645\n","   macro avg     0.9432    0.9387    0.9400      9645\n","weighted avg     0.9419    0.9404    0.9402      9645\n","\n","[[4928  137]\n"," [ 438 4142]]\n","SVM results on test set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8515    0.9099    0.8797      1254\n","         1.0     0.8946    0.8282    0.8601      1158\n","\n","    accuracy                         0.8706      2412\n","   macro avg     0.8730    0.8690    0.8699      2412\n","weighted avg     0.8722    0.8706    0.8703      2412\n","\n","[[1141  113]\n"," [ 199  959]]\n","\n","\n"," Fold: 2\n","Model 1, Total Epochs: 25, Train Accuracy: 0.943810906075057 Test Accuracy: 0.8797179593529656\n","Model 2, Total Epochs: 25, Train Accuracy: 0.9266017001866058 Test Accuracy: 0.8755703027789299\n","Model 1 and 2 results on train set:\n","Model 1 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9335    0.9624    0.9477      5103\n","         1.0     0.9562    0.9230    0.9393      4543\n","\n","    accuracy                         0.9438      9646\n","   macro avg     0.9448    0.9427    0.9435      9646\n","weighted avg     0.9442    0.9438    0.9437      9646\n","\n","[[4911  192]\n"," [ 350 4193]]\n","Model 2 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9322    0.9289    0.9305      5103\n","         1.0     0.9204    0.9241    0.9222      4543\n","\n","    accuracy                         0.9266      9646\n","   macro avg     0.9263    0.9265    0.9264      9646\n","weighted avg     0.9266    0.9266    0.9266      9646\n","\n","[[4740  363]\n"," [ 345 4198]]\n","Model 1 and 2 results on test set:\n","Model 1 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8651    0.9021    0.8833      1216\n","         1.0     0.8959    0.8569    0.8760      1195\n","\n","    accuracy                         0.8797      2411\n","   macro avg     0.8805    0.8795    0.8796      2411\n","weighted avg     0.8804    0.8797    0.8796      2411\n","\n","[[1097  119]\n"," [ 171 1024]]\n","Model 2 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8785    0.8742    0.8763      1216\n","         1.0     0.8726    0.8770    0.8748      1195\n","\n","    accuracy                         0.8756      2411\n","   macro avg     0.8756    0.8756    0.8756      2411\n","weighted avg     0.8756    0.8756    0.8756      2411\n","\n","[[1063  153]\n"," [ 147 1048]]\n","SVM results on train set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9323    0.9667    0.9492      5103\n","         1.0     0.9610    0.9212    0.9407      4543\n","\n","    accuracy                         0.9453      9646\n","   macro avg     0.9467    0.9439    0.9449      9646\n","weighted avg     0.9458    0.9453    0.9452      9646\n","\n","[[4933  170]\n"," [ 358 4185]]\n","SVM results on test set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8686    0.9079    0.8878      1216\n","         1.0     0.9018    0.8603    0.8805      1195\n","\n","    accuracy                         0.8843      2411\n","   macro avg     0.8852    0.8841    0.8842      2411\n","weighted avg     0.8850    0.8843    0.8842      2411\n","\n","[[1104  112]\n"," [ 167 1028]]\n","\n","\n"," Fold: 3\n","Model 1, Total Epochs: 25, Train Accuracy: 0.9335475844909807 Test Accuracy: 0.8681045209456657\n","Model 2, Total Epochs: 25, Train Accuracy: 0.9403897988803649 Test Accuracy: 0.8718374118622978\n","Model 1 and 2 results on train set:\n","Model 1 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9378    0.9346    0.9362      5030\n","         1.0     0.9290    0.9324    0.9307      4616\n","\n","    accuracy                         0.9335      9646\n","   macro avg     0.9334    0.9335    0.9334      9646\n","weighted avg     0.9336    0.9335    0.9336      9646\n","\n","[[4701  329]\n"," [ 312 4304]]\n","Model 2 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9226    0.9668    0.9442      5030\n","         1.0     0.9618    0.9116    0.9360      4616\n","\n","    accuracy                         0.9404      9646\n","   macro avg     0.9422    0.9392    0.9401      9646\n","weighted avg     0.9414    0.9404    0.9403      9646\n","\n","[[4863  167]\n"," [ 408 4208]]\n","Model 1 and 2 results on test set:\n","Model 1 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8808    0.8712    0.8760      1289\n","         1.0     0.8539    0.8645    0.8592      1122\n","\n","    accuracy                         0.8681      2411\n","   macro avg     0.8673    0.8679    0.8676      2411\n","weighted avg     0.8683    0.8681    0.8682      2411\n","\n","[[1123  166]\n"," [ 152  970]]\n","Model 2 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8587    0.9100    0.8836      1289\n","         1.0     0.8890    0.8280    0.8574      1122\n","\n","    accuracy                         0.8718      2411\n","   macro avg     0.8739    0.8690    0.8705      2411\n","weighted avg     0.8728    0.8718    0.8714      2411\n","\n","[[1173  116]\n"," [ 193  929]]\n","SVM results on train set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9267    0.9674    0.9466      5030\n","         1.0     0.9627    0.9166    0.9391      4616\n","\n","    accuracy                         0.9431      9646\n","   macro avg     0.9447    0.9420    0.9428      9646\n","weighted avg     0.9439    0.9431    0.9430      9646\n","\n","[[4866  164]\n"," [ 385 4231]]\n","SVM results on test set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8673    0.9123    0.8892      1289\n","         1.0     0.8929    0.8396    0.8654      1122\n","\n","    accuracy                         0.8785      2411\n","   macro avg     0.8801    0.8760    0.8773      2411\n","weighted avg     0.8792    0.8785    0.8781      2411\n","\n","[[1176  113]\n"," [ 180  942]]\n","\n","\n"," Fold: 4\n","Model 1, Total Epochs: 25, Train Accuracy: 0.9357246527057848 Test Accuracy: 0.8759850684363335\n","Model 2, Total Epochs: 25, Train Accuracy: 0.9285714285714286 Test Accuracy: 0.8772293654085441\n","Model 1 and 2 results on train set:\n","Model 1 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9186    0.9612    0.9394      5003\n","         1.0     0.9560    0.9082    0.9315      4643\n","\n","    accuracy                         0.9357      9646\n","   macro avg     0.9373    0.9347    0.9355      9646\n","weighted avg     0.9366    0.9357    0.9356      9646\n","\n","[[4809  194]\n"," [ 426 4217]]\n","Model 2 results on train set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9330    0.9290    0.9310      5003\n","         1.0     0.9239    0.9281    0.9260      4643\n","\n","    accuracy                         0.9286      9646\n","   macro avg     0.9284    0.9286    0.9285      9646\n","weighted avg     0.9286    0.9286    0.9286      9646\n","\n","[[4648  355]\n"," [ 334 4309]]\n","Model 1 and 2 results on test set:\n","Model 1 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8742    0.9027    0.8882      1316\n","         1.0     0.8783    0.8438    0.8607      1095\n","\n","    accuracy                         0.8760      2411\n","   macro avg     0.8762    0.8733    0.8745      2411\n","weighted avg     0.8761    0.8760    0.8757      2411\n","\n","[[1188  128]\n"," [ 171  924]]\n","Model 2 results on test set\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9003    0.8716    0.8857      1316\n","         1.0     0.8514    0.8840    0.8674      1095\n","\n","    accuracy                         0.8772      2411\n","   macro avg     0.8758    0.8778    0.8765      2411\n","weighted avg     0.8781    0.8772    0.8774      2411\n","\n","[[1147  169]\n"," [ 127  968]]\n","SVM results on train set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.9272    0.9552    0.9410      5003\n","         1.0     0.9501    0.9192    0.9344      4643\n","\n","    accuracy                         0.9379      9646\n","   macro avg     0.9387    0.9372    0.9377      9646\n","weighted avg     0.9383    0.9379    0.9379      9646\n","\n","[[4779  224]\n"," [ 375 4268]]\n","SVM results on test set:\n","              precision    recall  f1-score   support\n","\n","         0.0     0.8853    0.8974    0.8913      1316\n","         1.0     0.8747    0.8603    0.8674      1095\n","\n","    accuracy                         0.8805      2411\n","   macro avg     0.8800    0.8788    0.8794      2411\n","weighted avg     0.8805    0.8805    0.8805      2411\n","\n","[[1181  135]\n"," [ 153  942]]\n"]}],"source":["\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import KFold\n","from sklearn import svm\n","\n","class Trainer:\n","    def __init__(self):\n","        self.model1 = None\n","        self.model2 = None\n","        self.clf = None\n","        self.svmx_train = None\n","        self.svmy_train = None\n","        self.svmx_test = None\n","        self.svmy_test = None\n","\n","    def train_model1(self, train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata):\n","        te = 25\n","        self.model1 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","        self.model1.train()\n","        criterion = nn.BCELoss().to(device)\n","        optimizer = optim.Adam(self.model1.parameters(), lr=0.01)\n","\n","        self.model1.train()\n","        for epoch in range(te):\n","            optimizer.zero_grad()\n","            outputs = self.model1(train_claim.to(device))\n","            loss = criterion(outputs.squeeze(), y_traindata.to(device))\n","            loss.backward()\n","            optimizer.step()\n","\n","        self.model1.eval()\n","        with torch.no_grad():\n","            predictions_tr = self.model1(train_claim.to(device))\n","            predictions_tr = (predictions_tr > 0.5).to('cpu').int().squeeze().numpy()\n","        with torch.no_grad():\n","            predictions_te = self.model1(test_claim)\n","            predictions_te = (predictions_te > 0.5).to('cpu').int().squeeze().numpy()\n","\n","        print(f\"Model 1, Total Epochs: {te}, Train Accuracy: {metrics.accuracy_score(y_traindata.to('cpu'), predictions_tr)} Test Accuracy: {metrics.accuracy_score(y_testdata.to('cpu'), predictions_te)}\")\n","        \n","    \n","    def train_model2(self, train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata):\n","        te = 25\n","        self.model2 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","        self.model2.train()\n","        criterion = nn.BCELoss().to(device)\n","        optimizer = optim.Adam(self.model2.parameters(), lr=0.01)\n","\n","        self.model2.train()\n","        for epoch in range(te):\n","            optimizer.zero_grad()\n","            outputs = self.model2(train_ss.to(device))\n","            loss = criterion(outputs.squeeze(), y_traindata.to(device))\n","            loss.backward()\n","            optimizer.step()\n","\n","        self.model2.eval()\n","        with torch.no_grad():\n","            predictions_tr = self.model2(train_ss.to(device))\n","            predictions_tr = (predictions_tr > 0.5).to('cpu').int().squeeze().numpy()\n","        with torch.no_grad():\n","            predictions_te = self.model2(test_ss)\n","            predictions_te = (predictions_te > 0.5).to('cpu').int().squeeze().numpy()\n","\n","        print(f\"Model 2, Total Epochs: {te}, Train Accuracy: {metrics.accuracy_score(y_traindata.to('cpu'), predictions_tr)} Test Accuracy: {metrics.accuracy_score(y_testdata.to('cpu'), predictions_te)}\")\n","\n","        \n","    def report_stats_on_train_set_of_inidividual_models(self, train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata):\n","        ytr = []\n","        cury = y_traindata.detach().cpu().numpy()\n","        xtr1 = []\n","        xtr2 = []\n","        for i in range(len(cury)):\n","            ytr.append(cury[i])\n","            xtr1.append(train_claim[i].detach().cpu().numpy())\n","            xtr2.append(train_ss[i].detach().cpu().numpy())\n","\n","        self.model1.eval()\n","        self.model2.eval()\n","        with torch.no_grad():\n","            p1 = self.model1(torch.tensor(np.array(xtr1)).to(device))\n","            p1 = (p1 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","            p2 = self.model2(torch.tensor(np.array(xtr2)).to(device))\n","            p2 = (p2 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","\n","            Y_TRAIN = torch.tensor(np.array(ytr))\n","\n","            print(\"Model 1 results on train set\")\n","            print(classification_report(Y_TRAIN, p1, digits=4))\n","            print(confusion_matrix(Y_TRAIN, p1))\n","            print(\"Model 2 results on train set\")\n","            print(classification_report(Y_TRAIN, p2, digits=4))\n","            print(confusion_matrix(Y_TRAIN, p2))\n","            \n","    def report_stats_on_test_set_of_inidividual_models(self, train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata):\n","        ytr = []\n","        cury = y_testdata.detach().cpu().numpy()\n","        xtr1 = []\n","        xtr2 = []\n","        for i in range(len(cury)):\n","            ytr.append(cury[i])\n","            xtr1.append(test_claim[i].detach().cpu().numpy())\n","            xtr2.append(test_ss[i].detach().cpu().numpy())\n","\n","        self.model1.eval()\n","        self.model2.eval()\n","        with torch.no_grad():\n","            p1 = self.model1(torch.tensor(np.array(xtr1)).to(device))\n","            p1 = (p1 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","            p2 = self.model2(torch.tensor(np.array(xtr2)).to(device))\n","            p2 = (p2 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","\n","            Y_TEST = torch.tensor(np.array(ytr))\n","\n","            print(\"Model 1 results on test set\")\n","            print(classification_report(Y_TEST, p1, digits=4))\n","            print(confusion_matrix(Y_TEST, p1))\n","            print(\"Model 2 results on test set\")\n","            print(classification_report(Y_TEST, p2, digits=4))\n","            print(confusion_matrix(Y_TEST, p2))\n","            \n","            \n","    def train_test_svm(self, train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata, w1, w2):\n","        self.svmx_train = []\n","        self.svmy_train = []\n","        self.svmx_test = []\n","        self.svmy_test = []\n","\n","        ytr = []\n","        cury = y_traindata.detach().cpu().numpy()\n","        xtr1 = []\n","        xtr2 = []\n","        yte = []\n","        curyte = y_testdata.detach().cpu().numpy()\n","        xte1 = []\n","        xte2 = []\n","        for i in range(len(cury)):\n","            ytr.append(cury[i])\n","            xtr1.append(train_claim[i].detach().cpu().numpy())\n","            xtr2.append(train_ss[i].detach().cpu().numpy())\n","        for i in range(len(curyte)):\n","            yte.append(curyte[i])\n","            xte1.append(test_claim[i].detach().cpu().numpy())\n","            xte2.append(test_ss[i].detach().cpu().numpy())\n","\n","\n","        activation = {}\n","        def get_activation(name):\n","            def hook(model, input, output):\n","                activation[name] = output\n","            return hook\n","\n","        self.model1.bilstm2.register_forward_hook(get_activation('bilstm2_m1'))\n","        self.model1.eval()\n","        self.model2.bilstm2.register_forward_hook(get_activation('bilstm2_m2'))\n","        self.model2.eval()\n","        with torch.no_grad():\n","            p1 = self.model1(torch.tensor(np.array(xtr1)).to(device))\n","            p1 = (p1 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","            p2 = self.model2(torch.tensor(np.array(xtr2)).to(device))\n","            p2 = (p2 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","            weighted_avg = (w1 * activation['bilstm2_m1'][0][:, -1, :].clone().detach()) + (w2 * activation['bilstm2_m2'][0][:, -1, :].clone().detach())\n","            self.svmx_train = weighted_avg.detach().cpu().numpy()\n","            self.svmy_train = ytr\n","\n","            p1 = self.model1(torch.tensor(np.array(xte1)).to(device))\n","            p1 = (p1 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","            p2 = self.model2(torch.tensor(np.array(xte2)).to(device))\n","            p2 = (p2 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","            weighted_avg = (w1 * activation['bilstm2_m1'][0][:, -1, :].clone().detach()) + (w2* activation['bilstm2_m2'][0][:, -1, :].clone().detach())\n","            self.svmx_test = weighted_avg.detach().cpu().numpy()\n","            self.svmy_test = yte\n","\n","        self.clf = svm.SVC(probability=True, kernel='poly')\n","        self.clf.fit(self.svmx_train, self.svmy_train)\n","\n","        \n","    def report_stats_on_train_set_of_svm(self, train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata):\n","        preds = self.clf.predict(self.svmx_train)\n","        print(classification_report(self.svmy_train, preds, digits=4))\n","        print(confusion_matrix(self.svmy_train, preds))\n","        \n","    def report_stats_on_test_set_of_svm(self, train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata):\n","        preds = self.clf.predict(self.svmx_test)\n","        print(classification_report(self.svmy_test, preds, digits=4))\n","        print(confusion_matrix(self.svmy_test, preds))\n","        \n","    def train_all(self, train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata, w1, w2):\n","        self.train_model1(train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata)\n","        self.train_model2(train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata)\n","        self.train_test_svm(train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata, w1, w2)\n","        \n","    def test_all(self, train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata):\n","        print('Model 1 and 2 results on train set:')\n","        self.report_stats_on_train_set_of_inidividual_models(train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata)\n","        print('Model 1 and 2 results on test set:')\n","        self.report_stats_on_test_set_of_inidividual_models(train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata)\n","        print('SVM results on train set:')\n","        self.report_stats_on_train_set_of_svm(train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata)\n","        print('SVM results on test set:')\n","        self.report_stats_on_test_set_of_svm(train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata)\n","        \n","    def prepare_data(self, claim, ss, truths):\n","        xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n","        \n","        tk1 = text.Tokenizer(num_words=2000)\n","        tk2 = text.Tokenizer(num_words=2000)\n","        tk1.fit_on_texts(claim)\n","        tk2.fit_on_texts(ss)\n","        tokenized_train_claim = tk1.texts_to_sequences(claim)\n","        tokenized_train_ss = tk2.texts_to_sequences(ss)\n","        X_claim = torch.tensor(sequence.pad_sequences(tokenized_train_claim, maxlen=60)).to(device)\n","        X_ss = torch.tensor(sequence.pad_sequences(tokenized_train_ss, maxlen=60)).to(device)\n","\n","        y_labeldata = torch.tensor(truths.values).float().to(device)\n","\n","        return X_claim, X_ss, y_labeldata\n","    \n","    \n","\n","k = 5\n","kf = KFold(n_splits=k, shuffle=True)\n","\n","for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n","    print( '\\n\\n', 'Fold:', fold)\n","    trainer = Trainer()\n","    X_claim, X_ss, y_labeldata = trainer.prepare_data(X['claim'], X['simple_sentence'], y)\n","    \n","    train_claim = X_claim[train_idx]\n","    train_ss = X_claim[train_idx]\n","    test_claim = X_claim[val_idx]\n","    test_ss = X_claim[val_idx]\n","    y_traindata = y_labeldata[train_idx]\n","    y_testdata = y_labeldata[val_idx]\n","    \n","    trainer.train_all(train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata, 0.75, 0.25)\n","    trainer.test_all(train_claim, test_claim, train_ss, test_ss, y_traindata, y_testdata)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
