{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk==3.8.1 sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-25T17:45:42.840377Z","iopub.execute_input":"2023-02-25T17:45:42.841543Z","iopub.status.idle":"2023-02-25T17:45:58.881758Z","shell.execute_reply.started":"2023-02-25T17:45:42.841502Z","shell.execute_reply":"2023-02-25T17:45:58.880473Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting nltk==3.8.1\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hCollecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (1.2.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (2021.11.10)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (8.1.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (4.64.1)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.26.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.14.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=4f81cf1e9ac9967fd5fc07e5b8fa5b478d0ce37bcf1b244d86bc4380c627939e\n  Stored in directory: /root/.cache/pip/wheels/83/71/2b/40d17d21937fed496fb99145227eca8f20b4891240ff60c86f\nSuccessfully built sentence-transformers\nInstalling collected packages: nltk, sentence-transformers\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1 sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU, Dense\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import BatchNormalization\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom tensorflow.keras.preprocessing import sequence, text\nfrom tensorflow.keras.layers import Bidirectional, LSTM, Dense, Input, Embedding, Dropout, Conv1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:45:58.885521Z","iopub.execute_input":"2023-02-25T17:45:58.886058Z","iopub.status.idle":"2023-02-25T17:46:07.389789Z","shell.execute_reply.started":"2023-02-25T17:45:58.886010Z","shell.execute_reply":"2023-02-25T17:46:07.388699Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ru-ukr-onlyv4/ru-ukr-onlyv4.csv')\n\ndf.loc[df['truth_value'] == 'tom_ruling_pof', 'truth_value'] = 'meter-false'\ndf.loc[df['truth_value'] == 'meter-half-true', 'truth_value'] = 'meter-true'\ndf.loc[df['truth_value'] == 'meter-mostly-true', 'truth_value'] = 'meter-true'\ndf.loc[df['truth_value'] == 'meter-mostly-false', 'truth_value'] = 'meter-false'\ndf = df.dropna(subset=['claim'])\ndf.reset_index(drop=True, inplace=True)\ndf['truth_value'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:46:07.391290Z","iopub.execute_input":"2023-02-25T17:46:07.392090Z","iopub.status.idle":"2023-02-25T17:46:07.436163Z","shell.execute_reply.started":"2023-02-25T17:46:07.392050Z","shell.execute_reply":"2023-02-25T17:46:07.435066Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"meter-true     1358\nmeter-false    1325\nName: truth_value, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ncdf = df.copy()\ncdf['truth_value'] = le.fit_transform(cdf['truth_value'])","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:46:07.439183Z","iopub.execute_input":"2023-02-25T17:46:07.439565Z","iopub.status.idle":"2023-02-25T17:46:07.445675Z","shell.execute_reply.started":"2023-02-25T17:46:07.439531Z","shell.execute_reply":"2023-02-25T17:46:07.444542Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer(\"english\")\ndef stemm_text(text):\n    return ' '.join([stemmer.stem(w) for w in text.split(' ')])\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])\n\nT = cdf['claim'].str.split(' \\n\\n---\\n\\n').str[0]\nT = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\nstop = stopwords.words('english')\nT = T.apply(lambda x: ' '.join([y for y in x.split() if not y.isdigit()]))\nT = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\ncdf['claim'] = T\ncdf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:46:07.447220Z","iopub.execute_input":"2023-02-25T17:46:07.447929Z","iopub.status.idle":"2023-02-25T17:46:07.799125Z","shell.execute_reply.started":"2023-02-25T17:46:07.447889Z","shell.execute_reply":"2023-02-25T17:46:07.798045Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n  \n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                              claim  truth_value\n0           0                provocation disinformation overview            0\n1           1  ukraine theft homicide levels rose due power o...            0\n2           2  ukrainians beat two berlin residents speaking ...            0\n3           3                     quote paul goebbels banderites            0\n4           4  culture good neighborliness course ukrainian s...            0\n5           5  us research ukraine led increase incidence tic...            0\n6           6  chile law rights mutants genetically modified ...            0\n7           7          covid incidence rate became zero late may            0\n8           8  risk death among children vaccinated covid tim...            0\n9           9                provocation disinformation overview            0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>claim</th>\n      <th>truth_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>provocation disinformation overview</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ukraine theft homicide levels rose due power o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>ukrainians beat two berlin residents speaking ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>quote paul goebbels banderites</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>culture good neighborliness course ukrainian s...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>us research ukraine led increase incidence tic...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>chile law rights mutants genetically modified ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>covid incidence rate became zero late may</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>risk death among children vaccinated covid tim...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>provocation disinformation overview</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X = cdf['claim']\ny = cdf['truth_value']\n\nX = X.apply(lambda w: lemmatize_text(w))\nX","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:46:07.800503Z","iopub.execute_input":"2023-02-25T17:46:07.801141Z","iopub.status.idle":"2023-02-25T17:46:09.746574Z","shell.execute_reply.started":"2023-02-25T17:46:07.801103Z","shell.execute_reply":"2023-02-25T17:46:09.745526Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0                     provocation disinformation overview\n1       ukraine theft homicide level rose due power ou...\n2       ukrainian beat two berlin resident speaking ru...\n3                          quote paul goebbels banderites\n4       culture good neighborliness course ukrainian s...\n                              ...                        \n2678    global ballet community rally help young ukrai...\n2679    majority american say russia enemy differ hand...\n2680    china indicated le interest discussing violenc...\n2681    sanction punishing russia economy china econom...\n2682    thousand mile away ukraine russian invasion pl...\nName: claim, Length: 2683, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"torch.manual_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:46:09.747948Z","iopub.execute_input":"2023-02-25T17:46:09.748581Z","iopub.status.idle":"2023-02-25T17:46:09.803211Z","shell.execute_reply.started":"2023-02-25T17:46:09.748542Z","shell.execute_reply":"2023-02-25T17:46:09.802513Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def tokenizeAndGenerateSequences(X, y):\n    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    #Maximum number of characters in a sentence = 44\n    tk = text.Tokenizer(num_words=1000)\n    tk.fit_on_texts(xtrain)\n    tokenized_train = tk.texts_to_sequences(xtrain)\n    X_train = torch.tensor(sequence.pad_sequences(tokenized_train, maxlen=60)).to(device)\n    tokenized_test = tk.texts_to_sequences(xtest)\n    X_test = torch.tensor(sequence.pad_sequences(tokenized_test, maxlen=60)).to(device)\n\n    # Convert labels to tensors\n    y_train = torch.tensor(ytrain.values).float().to(device)\n    y_test = torch.tensor(ytest.values).float().to(device)\n    \n    return X_train, y_train, X_test, y_test\n\nX_train, y_train, X_test, y_test = tokenizeAndGenerateSequences(cdf['claim'], cdf['truth_value'])\nX_train, y_train, X_test, y_test = tokenizeAndGenerateSequences(X, y)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:46:09.804397Z","iopub.execute_input":"2023-02-25T17:46:09.804792Z","iopub.status.idle":"2023-02-25T17:46:13.855685Z","shell.execute_reply.started":"2023-02-25T17:46:09.804756Z","shell.execute_reply":"2023-02-25T17:46:13.854546Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(torch.Size([2146, 60]),\n torch.Size([2146]),\n torch.Size([537, 60]),\n torch.Size([537]))"},"metadata":{}}]},{"cell_type":"code","source":"\n# Define the model architecture\nclass BiLSTM(nn.Module):\n    def __init__(self, num_words, embed_size, hidden_size, output_size, dropout_rate):\n        super(BiLSTM, self).__init__()\n        self.embedding = nn.Embedding(num_words, embed_size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.bilstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        x, _ = self.bilstm1(x)\n        x, _ = self.bilstm2(x)\n        x = self.fc(x[:, -1, :])\n        x = self.sigmoid(x)\n        return x\n    \n\n# Train the model\nte = 50\nacc = []\ntracc = []\nfor e in range(1, te+1):\n    ctracc = 0\n    model = BiLSTM(num_words=1000, embed_size=60, hidden_size=64, output_size=1, dropout_rate=0.2).to(device)\n    criterion = nn.BCELoss().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n    model.train()\n    for epoch in range(e):\n        optimizer.zero_grad()\n        outputs = model(X_train.to(device))\n        loss = criterion(outputs.squeeze(), y_train)\n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            predictions = model(X_train.to(device))\n            predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n        train_accuracy = metrics.accuracy_score(y_train.to('cpu'), predictions)\n        ctracc += train_accuracy\n\n    ctracc /= e\n\n    # Evaluate the model\n    model.eval()\n    with torch.no_grad():\n        predictions = model(X_test)\n        predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n\n    print(f\"Total Epochs: {e}, Train Accuracy: {ctracc} Test Accuracy: {metrics.accuracy_score(y_test.to('cpu'), predictions)}\")\n    acc.append(metrics.accuracy_score(y_test.to('cpu'), predictions))\n    tracc.append(ctracc)\n#     break\nprint('Max acc -', max(acc), ' with epochs -', acc.index(max(acc)))\n# plt.plot([i for i in range(1, 51)], acc)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:46:13.857256Z","iopub.execute_input":"2023-02-25T17:46:13.858248Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Total Epochs: 1, Train Accuracy: 0.6766076421248836 Test Accuracy: 0.6629422718808193\nTotal Epochs: 2, Train Accuracy: 0.6593662628145387 Test Accuracy: 0.6256983240223464\nTotal Epochs: 3, Train Accuracy: 0.6545511028269648 Test Accuracy: 0.6405959031657356\nTotal Epochs: 4, Train Accuracy: 0.6826654240447344 Test Accuracy: 0.6964618249534451\nTotal Epochs: 5, Train Accuracy: 0.68928238583411 Test Accuracy: 0.6834264432029795\nTotal Epochs: 6, Train Accuracy: 0.6915191053122087 Test Accuracy: 0.7225325884543762\nTotal Epochs: 7, Train Accuracy: 0.7224071362002397 Test Accuracy: 0.7802607076350093\nTotal Epochs: 8, Train Accuracy: 0.7347972972972974 Test Accuracy: 0.776536312849162\nTotal Epochs: 9, Train Accuracy: 0.7580511546028788 Test Accuracy: 0.7746741154562383\nTotal Epochs: 10, Train Accuracy: 0.7870456663560111 Test Accuracy: 0.819366852886406\nTotal Epochs: 11, Train Accuracy: 0.7654833516902482 Test Accuracy: 0.7914338919925512\nTotal Epochs: 12, Train Accuracy: 0.766192917054986 Test Accuracy: 0.8175046554934823\nTotal Epochs: 13, Train Accuracy: 0.7866872177217006 Test Accuracy: 0.8286778398510242\nTotal Epochs: 14, Train Accuracy: 0.7882439089335642 Test Accuracy: 0.839851024208566\nTotal Epochs: 15, Train Accuracy: 0.8037278657968313 Test Accuracy: 0.8584729981378026\nTotal Epochs: 16, Train Accuracy: 0.7984913793103449 Test Accuracy: 0.851024208566108\nTotal Epochs: 17, Train Accuracy: 0.8193081519653527 Test Accuracy: 0.8677839851024208\nTotal Epochs: 18, Train Accuracy: 0.8197421559490525 Test Accuracy: 0.8752327746741154\nTotal Epochs: 19, Train Accuracy: 0.7979594839848922 Test Accuracy: 0.8324022346368715\nTotal Epochs: 20, Train Accuracy: 0.8404007455731595 Test Accuracy: 0.888268156424581\nTotal Epochs: 21, Train Accuracy: 0.8368393023565436 Test Accuracy: 0.8919925512104283\nTotal Epochs: 22, Train Accuracy: 0.8429424722528172 Test Accuracy: 0.888268156424581\nTotal Epochs: 23, Train Accuracy: 0.8563353458405933 Test Accuracy: 0.8752327746741154\nTotal Epochs: 24, Train Accuracy: 0.8625543647095372 Test Accuracy: 0.8696461824953445\nTotal Epochs: 25, Train Accuracy: 0.840093196644921 Test Accuracy: 0.8770949720670391\nTotal Epochs: 26, Train Accuracy: 0.858143952971539 Test Accuracy: 0.8864059590316573\nTotal Epochs: 27, Train Accuracy: 0.8685927306616963 Test Accuracy: 0.8677839851024208\nTotal Epochs: 28, Train Accuracy: 0.8448941552389829 Test Accuracy: 0.8808193668528864\nTotal Epochs: 29, Train Accuracy: 0.8798727383745221 Test Accuracy: 0.8677839851024208\nTotal Epochs: 30, Train Accuracy: 0.8685461323392357 Test Accuracy: 0.888268156424581\nTotal Epochs: 31, Train Accuracy: 0.8680215254186333 Test Accuracy: 0.8752327746741154\nTotal Epochs: 32, Train Accuracy: 0.8879019105312208 Test Accuracy: 0.8957169459962756\nTotal Epochs: 33, Train Accuracy: 0.893925273235618 Test Accuracy: 0.8994413407821229\nTotal Epochs: 34, Train Accuracy: 0.8921659996710707 Test Accuracy: 0.888268156424581\nTotal Epochs: 35, Train Accuracy: 0.8982425775529223 Test Accuracy: 0.8808193668528864\nTotal Epochs: 36, Train Accuracy: 0.9038650719685203 Test Accuracy: 0.8957169459962756\nTotal Epochs: 37, Train Accuracy: 0.8989446109669782 Test Accuracy: 0.8789571694599627\nTotal Epochs: 38, Train Accuracy: 0.9034311080590574 Test Accuracy: 0.8938547486033519\n","output_type":"stream"}]}]}