{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-16T08:49:01.831804Z","iopub.status.busy":"2023-04-16T08:49:01.831388Z","iopub.status.idle":"2023-04-16T08:49:16.552442Z","shell.execute_reply":"2023-04-16T08:49:16.551226Z","shell.execute_reply.started":"2023-04-16T08:49:01.831755Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nltk==3.8.1\n","  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (4.64.1)\n","Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (2021.11.10)\n","Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (8.1.3)\n","Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (1.2.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.27.4)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.13.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.14.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.13.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.4.0)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=933e3c0472d84f09156ebdb09aa08262c27269a1317f53957eb013de6c7e555c\n","  Stored in directory: /root/.cache/pip/wheels/83/71/2b/40d17d21937fed496fb99145227eca8f20b4891240ff60c86f\n","Successfully built sentence-transformers\n","Installing collected packages: nltk, sentence-transformers\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.4\n","    Uninstalling nltk-3.2.4:\n","      Successfully uninstalled nltk-3.2.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nltk-3.8.1 sentence-transformers-2.2.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install nltk==3.8.1 sentence-transformers\n","# !pip install pytorch2tikz"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:16.556267Z","iopub.status.busy":"2023-04-16T08:49:16.555625Z","iopub.status.idle":"2023-04-16T08:49:24.987402Z","shell.execute_reply":"2023-04-16T08:49:24.986244Z","shell.execute_reply.started":"2023-04-16T08:49:16.556221Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, GRU, Dense\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.layers import BatchNormalization\n","from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n","from tensorflow.keras.preprocessing import sequence, text\n","from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Input, Embedding, Dropout, Conv1D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.losses import BinaryCrossentropy\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:24.989562Z","iopub.status.busy":"2023-04-16T08:49:24.988779Z","iopub.status.idle":"2023-04-16T08:49:25.095827Z","shell.execute_reply":"2023-04-16T08:49:25.094816Z","shell.execute_reply.started":"2023-04-16T08:49:24.989521Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/latestones/latest3.csv')\n","\n","le = LabelEncoder()\n","cdf = df.copy()\n","cdf['truth_value'] = le.fit_transform(cdf['truth_value'])"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:25.099073Z","iopub.status.busy":"2023-04-16T08:49:25.098654Z","iopub.status.idle":"2023-04-16T08:49:25.117458Z","shell.execute_reply":"2023-04-16T08:49:25.116362Z","shell.execute_reply.started":"2023-04-16T08:49:25.099034Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["nan_mask = cdf['simple_sentence'].isna()\n","nan_indices = cdf[nan_mask].index\n","\n","# Replace NaN values in 'B' with values from column 'A'\n","cdf.loc[nan_indices, 'simple_sentence'] = df.loc[nan_indices, 'claim']\n","\n","cdf['simple_sentence'].isna().sum()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:25.120657Z","iopub.status.busy":"2023-04-16T08:49:25.119193Z","iopub.status.idle":"2023-04-16T08:49:26.771962Z","shell.execute_reply":"2023-04-16T08:49:26.770841Z","shell.execute_reply.started":"2023-04-16T08:49:25.120618Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n","  \n","/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>claim</th>\n","      <th>simple_sentence</th>\n","      <th>truth_value</th>\n","      <th>source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>ukraine theft homicide levels rose due power o...</td>\n","      <td>ukraine theft homicide levels rose due power o...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>ukrainians beat two berlin residents speaking ...</td>\n","      <td>ukrainians beat two berlin residents speaking ...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>quote paul goebbels banderites</td>\n","      <td>quote paul goebbels banderites</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>culture good neighborliness course ukrainian s...</td>\n","      <td>culture good neighborliness course ukrainian s...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>us research ukraine led increase incidence tic...</td>\n","      <td>us research ukraine led increase incidence tic...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>chile law rights mutants genetically modified ...</td>\n","      <td>chile law rights mutants approved ostap stakhi...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>covid incidence rate became zero late may</td>\n","      <td>covid incidence rate became zero late may</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>risk death among children vaccinated covid tim...</td>\n","      <td>risk death among children vaccinated covid tim...</td>\n","      <td>0</td>\n","      <td>vox-ukraine</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>russias army destroyed</td>\n","      <td>russias army destroyed</td>\n","      <td>0</td>\n","      <td>politifact</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>war ukraine</td>\n","      <td>war ukraine</td>\n","      <td>0</td>\n","      <td>politifact</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                              claim  \\\n","0           0  ukraine theft homicide levels rose due power o...   \n","1           1  ukrainians beat two berlin residents speaking ...   \n","2           2                     quote paul goebbels banderites   \n","3           3  culture good neighborliness course ukrainian s...   \n","4           4  us research ukraine led increase incidence tic...   \n","5           5  chile law rights mutants genetically modified ...   \n","6           6          covid incidence rate became zero late may   \n","7           7  risk death among children vaccinated covid tim...   \n","8           8                             russias army destroyed   \n","9           9                                        war ukraine   \n","\n","                                     simple_sentence  truth_value       source  \n","0  ukraine theft homicide levels rose due power o...            0  vox-ukraine  \n","1  ukrainians beat two berlin residents speaking ...            0  vox-ukraine  \n","2                     quote paul goebbels banderites            0  vox-ukraine  \n","3  culture good neighborliness course ukrainian s...            0  vox-ukraine  \n","4  us research ukraine led increase incidence tic...            0  vox-ukraine  \n","5  chile law rights mutants approved ostap stakhi...            0  vox-ukraine  \n","6          covid incidence rate became zero late may            0  vox-ukraine  \n","7  risk death among children vaccinated covid tim...            0  vox-ukraine  \n","8                             russias army destroyed            0   politifact  \n","9                                        war ukraine            0   politifact  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","\n","stemmer = SnowballStemmer(\"english\")\n","def stemm_text(text):\n","    return ' '.join([stemmer.stem(w) for w in text.split(' ')])\n","\n","w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n","lemmatizer = nltk.stem.WordNetLemmatizer()\n","\n","def lemmatize_text(text):\n","    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])\n","\n","T = cdf['claim'].str.split(' \\n\\n---\\n\\n').str[0]\n","T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n","stop = stopwords.words('english')\n","T = T.apply(lambda x: ' '.join([y for y in x.split() if not y.isdigit()]))\n","T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n","cdf['claim'] = T\n","\n","T = cdf['simple_sentence'].str.split(' \\n\\n---\\n\\n').str[0]\n","T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n","stop = stopwords.words('english')\n","T = T.apply(lambda x: ' '.join([y for y in x.split() if not y.isdigit()]))\n","T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n","cdf['simple_sentence'] = T\n","cdf.head(10)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:26.773942Z","iopub.status.busy":"2023-04-16T08:49:26.773547Z","iopub.status.idle":"2023-04-16T08:49:26.883769Z","shell.execute_reply":"2023-04-16T08:49:26.882772Z","shell.execute_reply.started":"2023-04-16T08:49:26.773902Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('wordnet')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:26.887185Z","iopub.status.busy":"2023-04-16T08:49:26.886888Z","iopub.status.idle":"2023-04-16T08:49:26.914592Z","shell.execute_reply":"2023-04-16T08:49:26.913529Z","shell.execute_reply.started":"2023-04-16T08:49:26.887157Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(   Unnamed: 0                                              claim  \\\n"," 0        9384  rocket attack mall kremenchuk provocation arme...   \n"," 1        8990  ortega denies torture nicaraguas prisons publi...   \n"," 2       11921                 yale shut art history course white   \n"," 3        8548                   lockdown ukraine extended spring   \n"," 4       10908  us eu organised coup détat kyiv creating extre...   \n"," \n","                                      simple_sentence  truth_value       source  \n"," 0  rocket attack mall kremenchuk provocation arme...            0     stopfake  \n"," 1  ortega denies torture nicaraguas prisons publi...            0    polygraph  \n"," 2                 yale shut art history course white            0     stopfake  \n"," 3                   lockdown ukraine extended spring            0     stopfake  \n"," 4  us organised coup détat kyiv creating extremel...            0  EUvsDisinfo  ,\n"," 0    566\n"," 1    566\n"," Name: truth_value, dtype: int64)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["adf = cdf.copy()\n","grouped = adf.groupby('truth_value')\n","\n","# Sample 900 rows from each group\n","sampled = grouped.apply(lambda x: x.sample(n=566))\n","\n","# Reset the index of the sampled data\n","sampled = sampled.reset_index(drop=True)\n","sampled.head(), sampled['truth_value'].value_counts()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:26.916634Z","iopub.status.busy":"2023-04-16T08:49:26.916005Z","iopub.status.idle":"2023-04-16T08:49:29.573370Z","shell.execute_reply":"2023-04-16T08:49:29.572306Z","shell.execute_reply.started":"2023-04-16T08:49:26.916594Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  import sys\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>claim</th>\n","      <th>simple_sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>taiwan chinese two millennium let check history</td>\n","      <td>taiwan chinese two millennium let check history</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>london helping kyiv turn country nuclear waste...</td>\n","      <td>london helping kyiv turn country nuclear waste...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>official rudy giuliani officially resigns trum...</td>\n","      <td>official rudy giuliani officially resigns trum...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>oleksiy arestovych adviser president zelensky ...</td>\n","      <td>oleksiy arestovych adviser president zelensky ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>putin situation ukraine civilized choice</td>\n","      <td>putin situation ukraine civilized choice</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               claim  \\\n","0    taiwan chinese two millennium let check history   \n","1  london helping kyiv turn country nuclear waste...   \n","2  official rudy giuliani officially resigns trum...   \n","3  oleksiy arestovych adviser president zelensky ...   \n","4           putin situation ukraine civilized choice   \n","\n","                                     simple_sentence  \n","0    taiwan chinese two millennium let check history  \n","1  london helping kyiv turn country nuclear waste...  \n","2  official rudy giuliani officially resigns trum...  \n","3  oleksiy arestovych adviser president zelensky ...  \n","4           putin situation ukraine civilized choice  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["cdf = cdf.sample(frac=1).reset_index(drop=True)\n","X = cdf[['claim', 'simple_sentence']]\n","y = cdf['truth_value']\n","\n","# X = X.apply(lambda w: lemmatize_text(w))\n","X['claim'] = X['claim'].apply(lambda w: lemmatize_text(w))\n","X['simple_sentence'] = X['simple_sentence'].apply(lambda w: lemmatize_text(w))\n","X.head()"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T07:06:46.172207Z","iopub.status.busy":"2023-04-16T07:06:46.171738Z","iopub.status.idle":"2023-04-16T07:06:47.166341Z","shell.execute_reply":"2023-04-16T07:06:47.165116Z","shell.execute_reply.started":"2023-04-16T07:06:46.172167Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘/root/nltk_data’: File exists\n"]}],"source":["!mkdir /root/nltk_data"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T07:06:47.168991Z","iopub.status.busy":"2023-04-16T07:06:47.168548Z","iopub.status.idle":"2023-04-16T07:06:47.199138Z","shell.execute_reply":"2023-04-16T07:06:47.197779Z","shell.execute_reply.started":"2023-04-16T07:06:47.168944Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to ./wordnet...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('wordnet', download_dir='./wordnet')"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T07:06:47.201293Z","iopub.status.busy":"2023-04-16T07:06:47.200911Z","iopub.status.idle":"2023-04-16T07:17:28.799036Z","shell.execute_reply":"2023-04-16T07:17:28.797785Z","shell.execute_reply.started":"2023-04-16T07:06:47.201256Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  ./wordnet/corpora/wordnet.zip\n","replace /root/nltk_data/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"]}],"source":["!unzip -o ./wordnet/corpora/wordnet.zip -d /root/nltk_data/corpora/ "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:29.575871Z","iopub.status.busy":"2023-04-16T08:49:29.575126Z","iopub.status.idle":"2023-04-16T08:49:29.648343Z","shell.execute_reply":"2023-04-16T08:49:29.647233Z","shell.execute_reply.started":"2023-04-16T08:49:29.575831Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device('cpu')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:29.653037Z","iopub.status.busy":"2023-04-16T08:49:29.652082Z","iopub.status.idle":"2023-04-16T08:49:35.003886Z","shell.execute_reply":"2023-04-16T08:49:35.002753Z","shell.execute_reply.started":"2023-04-16T08:49:29.652993Z"},"trusted":true},"outputs":[],"source":["def tokenizeAndGenerateSequences(X, y):\n","    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n","\n","    tk1 = text.Tokenizer(num_words=2000)\n","    tk2 = text.Tokenizer(num_words=2000)\n","    tk1.fit_on_texts(xtrain['claim'])\n","    tk2.fit_on_texts(xtrain['simple_sentence'])\n","    tokenized_train_claim = tk1.texts_to_sequences(xtrain['claim'])\n","    tokenized_train_ss = tk2.texts_to_sequences(xtrain['simple_sentence'])\n","    X_train_claim = torch.tensor(sequence.pad_sequences(tokenized_train_claim, maxlen=60)).to(device)\n","    X_train_ss = torch.tensor(sequence.pad_sequences(tokenized_train_ss, maxlen=60)).to(device)\n","    tokenized_test_claim = tk1.texts_to_sequences(xtest['claim'])\n","    tokenized_test_ss = tk1.texts_to_sequences(xtest['simple_sentence'])\n","    X_test_claim = torch.tensor(sequence.pad_sequences(tokenized_test_claim, maxlen=60)).to(device)\n","    X_test_ss = torch.tensor(sequence.pad_sequences(tokenized_test_ss, maxlen=60)).to(device)\n","\n","    # Convert labels to tensors\n","    y_train = torch.tensor(ytrain.values).float().to(device)\n","    y_test = torch.tensor(ytest.values).float().to(device)\n","    \n","    return X_train_claim, X_train_ss, y_train, X_test_claim, X_test_ss, y_test\n","\n","# X_train_txt, y_train_txt, X_test_txt, y_test_txt = xtrain, xtest, ytrain, ytest = train_test_split(cdf['claim'], cdf['truth_value'], test_size=0.2, random_state=42)\n","X_train_claim, X_train_ss, y_train, X_test_claim, X_test_ss, y_test = tokenizeAndGenerateSequences(X, y)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:35.015433Z","iopub.status.busy":"2023-04-16T08:49:35.014395Z","iopub.status.idle":"2023-04-16T08:49:35.030912Z","shell.execute_reply":"2023-04-16T08:49:35.029716Z","shell.execute_reply.started":"2023-04-16T08:49:35.015395Z"},"trusted":true},"outputs":[],"source":["\n","# Define the model architecture\n","class BiLSTM(nn.Module):\n","    def __init__(self, num_words, embed_size, hidden_size, fc_out_size, output_size, dropout_rate):\n","        super(BiLSTM, self).__init__()\n","        self.embedding = nn.Embedding(num_words, embed_size)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.bilstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n","        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n","#         self.fc = nn.Linear(hidden_size * 2, output_size)\n","        self.fc = nn.Linear(hidden_size * 2, fc_out_size)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.bilstm1(x)\n","        x = self.dropout(x)\n","        x, _ = self.bilstm2(x)\n","        x = self.dropout(x)\n","        x = self.fc(x[:, -1, :])\n","#         x = self.fc(torch.flatten(x, start_dim=1))\n","        return x\n","    \n","\n","# plt.plot([i for i in range(1, 51)], acc)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:35.033395Z","iopub.status.busy":"2023-04-16T08:49:35.032441Z","iopub.status.idle":"2023-04-16T08:49:35.045474Z","shell.execute_reply":"2023-04-16T08:49:35.044437Z","shell.execute_reply.started":"2023-04-16T08:49:35.033353Z"},"trusted":true},"outputs":[],"source":["\n","# Define the model architecture\n","class BiLSTM2(nn.Module):\n","    def __init__(self, num_words, embed_size, hidden_size, fc_out_size, output_size, dropout_rate):\n","        super(BiLSTM2, self).__init__()\n","        self.embedding = nn.Embedding(num_words, embed_size)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.bilstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n","        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n","#         self.fc = nn.Linear(hidden_size * 2, output_size)\n","        self.fc = nn.Linear(hidden_size * 2, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.bilstm1(x)\n","        x = self.dropout(x)\n","        x, _ = self.bilstm2(x)\n","        x = self.dropout(x)\n","        x = self.fc(x[:, -1, :])\n","#         x = self.fc(torch.flatten(x, start_dim=1))\n","        return self.sigmoid(x)\n","    \n","\n","# plt.plot([i for i in range(1, 51)], acc)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:35.047544Z","iopub.status.busy":"2023-04-16T08:49:35.047101Z","iopub.status.idle":"2023-04-16T08:49:35.058473Z","shell.execute_reply":"2023-04-16T08:49:35.057446Z","shell.execute_reply.started":"2023-04-16T08:49:35.047506Z"},"trusted":true},"outputs":[],"source":["class TreeBiLSTM(nn.Module):\n","    def __init__(self, num_words, embed_size, hidden_size, fc_out_size, output_size, dropout_rate):\n","        super(TreeBiLSTM, self).__init__()\n","        self.model1 = BiLSTM(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","        self.model2 = BiLSTM(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","        self.fc1 = nn.Linear(fc_out_size * 2, 1024)\n","        self.fc2 = nn.Linear(1024, 256)\n","        self.fc3 = nn.Linear(256, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        a = self.model1(x[0])\n","        b = self.model2(x[1])\n","        \n","        res = torch.cat((a, b), 1)\n","#         print(a.shape, b.shape, res.shape)\n","        res = self.fc1(res)\n","        res = self.fc2(res)\n","        res = self.fc3(res)\n","        res = self.sigmoid(res)\n","        return res      \n","        \n","        "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:49:35.060286Z","iopub.status.busy":"2023-04-16T08:49:35.059916Z","iopub.status.idle":"2023-04-16T08:50:13.211933Z","shell.execute_reply":"2023-04-16T08:50:13.210931Z","shell.execute_reply.started":"2023-04-16T08:49:35.060247Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Epochs: 50, Train Accuracy: 0.6848937273198545 Test Accuracy: 0.4701492537313433\n"]}],"source":["\n","# Train the model\n","te = 50\n","acc = []\n","tracc = []\n","dataset = torch.utils.data.TensorDataset(X_train_claim, y_train)\n","dataloader = torch.utils.data.DataLoader(\n","    dataset, batch_size=1024, shuffle=True)\n","for e in range(te, te+1):\n","    ctracc = 0\n","    model = TreeBiLSTM(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","    criterion = nn.BCELoss().to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","    model.train()\n","    curtraacc = []\n","    for epoch in range(e):\n","        optimizer.zero_grad()\n","        outputs = model([X_train_claim.to(device), X_train_ss.to(device)])\n","        loss = criterion(outputs.squeeze(), y_train.to(device))\n","#         losses.append(loss)\n","        loss.backward()\n","        optimizer.step()\n","\n","        with torch.no_grad():\n","            predictions = model([X_train_claim.to(device), X_train_ss.to(device)])\n","            predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n","        train_accuracy = metrics.accuracy_score(y_train.to(device).to('cpu'), predictions)\n","        ctracc += train_accuracy\n","        curtraacc.append(train_accuracy)\n","\n","    ctracc /= e\n","\n","    # Evaluate the model\n","    model.eval()\n","    with torch.no_grad():\n","        predictions = model([X_test_claim.to(device), X_test_ss.to(device)])\n","        predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n","\n","    print(f\"Total Epochs: {e}, Train Accuracy: {ctracc} Test Accuracy: {metrics.accuracy_score(y_test.to('cpu'), predictions)}\")\n","    acc.append(metrics.accuracy_score(y_test.to('cpu'), predictions))\n","    tracc.append(ctracc)\n","#     break\n","#     plt.plot([i for i in range(a)], curtraacc)\n","    torch.save(model.state_dict(), 'arch1.pt')\n","# print('Max acc -', max(acc), ' with epochs -', acc.index(max(acc)))"]},{"cell_type":"code","execution_count":168,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T11:45:17.006908Z","iopub.status.busy":"2023-04-16T11:45:17.006425Z","iopub.status.idle":"2023-04-16T11:45:17.015006Z","shell.execute_reply":"2023-04-16T11:45:17.013971Z","shell.execute_reply.started":"2023-04-16T11:45:17.006866Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([9645, 60])"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["X_train_claim.shape"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:50:13.218569Z","iopub.status.busy":"2023-04-16T08:50:13.216282Z","iopub.status.idle":"2023-04-16T08:50:13.296000Z","shell.execute_reply":"2023-04-16T08:50:13.295058Z","shell.execute_reply.started":"2023-04-16T08:50:13.218529Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.00      0.00      0.00      1278\n","         1.0       0.47      1.00      0.64      1134\n","\n","    accuracy                           0.47      2412\n","   macro avg       0.24      0.50      0.32      2412\n","weighted avg       0.22      0.47      0.30      2412\n","\n","[[   0 1278]\n"," [   0 1134]]\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","\n","model.eval()\n","with torch.no_grad():\n","    predictions = model([X_test_claim.to(device), X_test_ss.to(device)])\n","    predictions = (predictions > 0.95).to('cpu').int().squeeze().numpy()\n","    Y_TEST = y_test.to('cpu')\n","    \n","    \n","    print(classification_report(Y_TEST, predictions))\n","    print(confusion_matrix(Y_TEST, predictions))"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:50:13.304381Z","iopub.status.busy":"2023-04-16T08:50:13.302030Z","iopub.status.idle":"2023-04-16T08:50:20.501048Z","shell.execute_reply":"2023-04-16T08:50:20.499089Z","shell.execute_reply.started":"2023-04-16T08:50:13.304339Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Epochs: 25, Train Accuracy: 0.8511850699844479 Test Accuracy: 0.8598673300165838\n","Max acc - 0.8598673300165838  with epochs - 0\n"]}],"source":["\n","# Train the model\n","te = 25\n","acc = []\n","tracc = []\n","dataset = torch.utils.data.TensorDataset(X_train_ss, y_train)\n","dataloader = torch.utils.data.DataLoader(\n","    dataset, batch_size=1024, shuffle=True)\n","curtraacc = []\n","model1 = None\n","for e in range(te, te+1):\n","    ctracc = 0\n","    model1 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","    model1.train()\n","    criterion = nn.BCELoss().to(device)\n","    optimizer = optim.Adam(model1.parameters(), lr=0.01)\n","\n","    model1.train()\n","    for epoch in range(e):\n","        optimizer.zero_grad()\n","        outputs = model1(X_train_claim.to(device))\n","        loss = criterion(outputs.squeeze(), y_train.to(device))\n","        loss.backward()\n","        optimizer.step()\n","\n","        with torch.no_grad():\n","            predictions = model1(X_train_claim.to(device))\n","            predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n","        train_accuracy = metrics.accuracy_score(y_train.to(device).to('cpu'), predictions)\n","        ctracc += train_accuracy\n","        curtraacc.append(train_accuracy)\n","\n","    ctracc /= e\n","\n","    # Evaluate the model\n","    model1.eval()\n","    with torch.no_grad():\n","        predictions = model1(X_test_ss)\n","        predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n","\n","    print(f\"Total Epochs: {e}, Train Accuracy: {ctracc} Test Accuracy: {metrics.accuracy_score(y_test.to('cpu'), predictions)}\")\n","    acc.append(metrics.accuracy_score(y_test.to('cpu'), predictions))\n","    tracc.append(ctracc)\n","#     plt.plot([i for i in range(te)], curtraacc)\n","#     break\n","print('Max acc -', max(acc), ' with epochs -', acc.index(max(acc)))"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:50:20.503065Z","iopub.status.busy":"2023-04-16T08:50:20.502659Z","iopub.status.idle":"2023-04-16T08:50:27.690099Z","shell.execute_reply":"2023-04-16T08:50:27.688775Z","shell.execute_reply.started":"2023-04-16T08:50:20.503025Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Epochs: 25, Train Accuracy: 0.8559709694142044 Test Accuracy: 0.7023217247097844\n","Max acc - 0.7023217247097844  with epochs - 0\n"]}],"source":["\n","# Train the model\n","te = 25\n","acc = []\n","tracc = []\n","dataset = torch.utils.data.TensorDataset(X_train_ss, y_train)\n","dataloader = torch.utils.data.DataLoader(\n","    dataset, batch_size=1024, shuffle=True)\n","curtraacc = []\n","model2 = None\n","for e in range(te, te+1):\n","    ctracc = 0\n","    model2 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","    model2.train()\n","    criterion = nn.BCELoss().to(device)\n","    optimizer = optim.Adam(model2.parameters(), lr=0.01)\n","\n","    model2.train()\n","    for epoch in range(e):\n","        optimizer.zero_grad()\n","        outputs = model2(X_train_ss.to(device))\n","        loss = criterion(outputs.squeeze(), y_train.to(device))\n","        loss.backward()\n","        optimizer.step()\n","\n","        with torch.no_grad():\n","            predictions = model2(X_train_ss.to(device))\n","            predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n","        train_accuracy = metrics.accuracy_score(y_train.to(device).to('cpu'), predictions)\n","        ctracc += train_accuracy\n","        curtraacc.append(train_accuracy)\n","\n","    ctracc /= e\n","\n","    # Evaluate the model\n","    model2.eval()\n","    with torch.no_grad():\n","        predictions = model2(X_test_ss)\n","        predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n","\n","    print(f\"Total Epochs: {e}, Train Accuracy: {ctracc} Test Accuracy: {metrics.accuracy_score(y_test.to('cpu'), predictions)}\")\n","    acc.append(metrics.accuracy_score(y_test.to('cpu'), predictions))\n","    tracc.append(ctracc)\n","#     plt.plot([i for i in range(te)], curtraacc)\n","#     break\n","print('Max acc -', max(acc), ' with epochs -', acc.index(max(acc)))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T08:50:27.692030Z","iopub.status.busy":"2023-04-16T08:50:27.691649Z","iopub.status.idle":"2023-04-16T08:50:27.706322Z","shell.execute_reply":"2023-04-16T08:50:27.705342Z","shell.execute_reply.started":"2023-04-16T08:50:27.691996Z"},"trusted":true},"outputs":[],"source":["torch.save(model1.state_dict(), '/kaggle/working/raw.pt')\n","torch.save(model2.state_dict(), '/kaggle/working/ss.pt')"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T09:01:18.110034Z","iopub.status.busy":"2023-04-16T09:01:18.109553Z","iopub.status.idle":"2023-04-16T09:01:20.418560Z","shell.execute_reply":"2023-04-16T09:01:20.417503Z","shell.execute_reply.started":"2023-04-16T09:01:18.109992Z"},"trusted":true},"outputs":[],"source":["ytf = []\n","yyyyy = y_test.detach().cpu().numpy()\n","xtct = []\n","for i in range(len(yyyyy)):\n","#     if yyyyy[i] == 0:\n","    ytf.append(yyyyy[i])\n","    xtct.append(X_test_claim[i].detach().cpu().numpy())"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T09:01:20.420953Z","iopub.status.busy":"2023-04-16T09:01:20.420552Z","iopub.status.idle":"2023-04-16T09:01:20.494565Z","shell.execute_reply":"2023-04-16T09:01:20.493379Z","shell.execute_reply.started":"2023-04-16T09:01:20.420915Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.87      0.88      0.87      1278\n","         1.0       0.86      0.85      0.85      1134\n","\n","    accuracy                           0.86      2412\n","   macro avg       0.86      0.86      0.86      2412\n","weighted avg       0.86      0.86      0.86      2412\n","\n","[[1125  153]\n"," [ 174  960]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.72      0.72      0.72      1278\n","         1.0       0.69      0.69      0.69      1134\n","\n","    accuracy                           0.71      2412\n","   macro avg       0.70      0.70      0.70      2412\n","weighted avg       0.71      0.71      0.71      2412\n","\n","[[919 359]\n"," [352 782]]\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","\n","model1.eval()\n","with torch.no_grad():\n","    predictions = model1(torch.tensor(np.array(xtct)).to(device))\n","    predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n","    Y_TEST = torch.tensor(np.array(ytf))\n","    \n","\n","    print(classification_report(Y_TEST, predictions))\n","    print(confusion_matrix(Y_TEST, predictions))\n","    \n","\n","model2.eval()\n","with torch.no_grad():\n","    predictions = model2(torch.tensor(np.array(xtct)).to(device))\n","    predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n","    Y_TEST = torch.tensor(np.array(ytf))\n","    \n","\n","    print(classification_report(Y_TEST, predictions))\n","    print(confusion_matrix(Y_TEST, predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-15T19:23:01.032995Z","iopub.status.idle":"2023-04-15T19:23:01.033819Z","shell.execute_reply":"2023-04-15T19:23:01.033585Z","shell.execute_reply.started":"2023-04-15T19:23:01.033559Z"},"trusted":true},"outputs":[],"source":["!pip show tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-15T19:23:01.035223Z","iopub.status.idle":"2023-04-15T19:23:01.036087Z","shell.execute_reply":"2023-04-15T19:23:01.035854Z","shell.execute_reply.started":"2023-04-15T19:23:01.035828Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'rs.pt')"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T09:01:41.454662Z","iopub.status.busy":"2023-04-16T09:01:41.454062Z","iopub.status.idle":"2023-04-16T09:01:42.235889Z","shell.execute_reply":"2023-04-16T09:01:42.234233Z","shell.execute_reply.started":"2023-04-16T09:01:41.454625Z"},"trusted":true},"outputs":[],"source":["def tokenizeAndGenerateSequences(X, y):\n","    tk1 = text.Tokenizer(num_words=2000)\n","    tk2 = text.Tokenizer(num_words=2000)\n","    tk1.fit_on_texts(X['claim'])\n","    tk2.fit_on_texts(X['simple_sentence'])\n","    tokenized_train_claim = tk1.texts_to_sequences(X['claim'])\n","    tokenized_train_ss = tk2.texts_to_sequences(X['simple_sentence'])\n","    X = torch.tensor(sequence.pad_sequences(tokenized_train_claim, maxlen=60)).to(device)\n","    X = torch.tensor(sequence.pad_sequences(tokenized_train_ss, maxlen=60)).to(device)\n","\n","    # Convert labels to tensors\n","    y = torch.tensor(y.values).float().to(device)\n","    \n","    return X, y\n","\n","# X_train_txt, y_train_txt, X_test_txt, y_test_txt = xtrain, xtest, ytrain, ytest = train_test_split(cdf['claim'], cdf['truth_value'], test_size=0.2, random_state=42)\n","nx, ny = tokenizeAndGenerateSequences(X, y)"]},{"cell_type":"code","execution_count":177,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:13:51.134955Z","iopub.status.busy":"2023-04-16T12:13:51.134179Z","iopub.status.idle":"2023-04-16T12:13:51.161713Z","shell.execute_reply":"2023-04-16T12:13:51.160512Z","shell.execute_reply.started":"2023-04-16T12:13:51.134917Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":["m1 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","m2 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","s1 = torch.load('/kaggle/working/raw.pt')\n","s2 = torch.load('/kaggle/working/ss.pt')\n","m1.load_state_dict(s1)\n","m2.load_state_dict(s2)"]},{"cell_type":"code","execution_count":178,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:13:52.747371Z","iopub.status.busy":"2023-04-16T12:13:52.746217Z","iopub.status.idle":"2023-04-16T12:13:53.613254Z","shell.execute_reply":"2023-04-16T12:13:53.612094Z","shell.execute_reply.started":"2023-04-16T12:13:52.747322Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94      5041\n","         1.0       0.94      0.92      0.93      4604\n","\n","    accuracy                           0.93      9645\n","   macro avg       0.94      0.93      0.93      9645\n","weighted avg       0.94      0.93      0.93      9645\n","\n","[[4775  266]\n"," [ 361 4243]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.95      0.94      5041\n","         1.0       0.94      0.93      0.94      4604\n","\n","    accuracy                           0.94      9645\n","   macro avg       0.94      0.94      0.94      9645\n","weighted avg       0.94      0.94      0.94      9645\n","\n","[[4768  273]\n"," [ 303 4301]]\n"]}],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","svmx = []\n","svmy = []\n","\n","ytr = []\n","cury = y_train.detach().cpu().numpy()\n","xtr1 = []\n","xtr2 = []\n","for i in range(len(cury)):\n","#     if yyyyy[i] == 0:\n","    ytr.append(cury[i])\n","    xtr1.append(X_train_claim[i].detach().cpu().numpy())\n","    xtr2.append(X_train_ss[i].detach().cpu().numpy())\n","#     svmx.append(X_train_ss[i].detach().cpu().numpy().tolist())\n","    \n","m1.eval()\n","m2.eval()\n","with torch.no_grad():\n","    p1 = m1(torch.tensor(np.array(xtr1)).to(device))\n","    p1 = (p1 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","    p2 = m2(torch.tensor(np.array(xtr2)).to(device))\n","    p2 = (p2 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","#     aaa = X_train_ss[i].detach().cpu().numpy().tolist().copy()\n","    svmx = [X_train_claim[i].detach().cpu().numpy().tolist() + [p1[i], p2[i]] for i in range(len(p1))]\n","    \n","    Y_TRAIN = torch.tensor(np.array(ytr))\n","    \n","\n","    print(classification_report(Y_TRAIN, p1))\n","    print(confusion_matrix(Y_TRAIN, p1))\n","    print(classification_report(Y_TRAIN, p2))\n","    print(confusion_matrix(Y_TRAIN, p2))"]},{"cell_type":"code","execution_count":179,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:14:19.360094Z","iopub.status.busy":"2023-04-16T12:14:19.359437Z","iopub.status.idle":"2023-04-16T12:14:19.614014Z","shell.execute_reply":"2023-04-16T12:14:19.612825Z","shell.execute_reply.started":"2023-04-16T12:14:19.360057Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.87      0.88      0.87      1278\n","         1.0       0.86      0.85      0.85      1134\n","\n","    accuracy                           0.86      2412\n","   macro avg       0.86      0.86      0.86      2412\n","weighted avg       0.86      0.86      0.86      2412\n","\n","[[1125  153]\n"," [ 174  960]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.73      0.69      0.71      1278\n","         1.0       0.67      0.72      0.69      1134\n","\n","    accuracy                           0.70      2412\n","   macro avg       0.70      0.70      0.70      2412\n","weighted avg       0.70      0.70      0.70      2412\n","\n","[[877 401]\n"," [317 817]]\n"]}],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","svmx = []\n","svmy = []\n","\n","ytr = []\n","cury = y_test.detach().cpu().numpy()\n","xtr1 = []\n","xtr2 = []\n","for i in range(len(cury)):\n","#     if yyyyy[i] == 0:\n","    ytr.append(cury[i])\n","    xtr1.append(X_test_claim[i].detach().cpu().numpy())\n","    xtr2.append(X_test_ss[i].detach().cpu().numpy())\n","#     svmx.append(X_train_ss[i].detach().cpu().numpy().tolist())\n","    \n","m1.eval()\n","m2.eval()\n","with torch.no_grad():\n","    p1 = m1(torch.tensor(np.array(xtr1)).to(device))\n","    p1 = (p1 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","    p2 = m2(torch.tensor(np.array(xtr2)).to(device))\n","    p2 = (p2 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","#     aaa = X_train_ss[i].detach().cpu().numpy().tolist().copy()\n","    svmx = [X_test_claim[i].detach().cpu().numpy().tolist() + [p1[i], p2[i]] for i in range(len(p1))]\n","    \n","    Y_TEST = torch.tensor(np.array(ytr))\n","    \n","\n","    print(classification_report(Y_TEST, p1))\n","    print(confusion_matrix(Y_TEST, p1))\n","    print(classification_report(Y_TEST, p2))\n","    print(confusion_matrix(Y_TEST, p2))"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T09:05:49.643643Z","iopub.status.busy":"2023-04-16T09:05:49.643033Z","iopub.status.idle":"2023-04-16T09:06:18.204009Z","shell.execute_reply":"2023-04-16T09:06:18.202812Z","shell.execute_reply.started":"2023-04-16T09:05:49.643594Z"},"trusted":true},"outputs":[{"data":{"text/plain":["SVC(probability=True)"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn import svm\n","\n","clf = svm.SVC(probability=True)\n","clf.fit(svmx, ytr)\n","preds = clf.predict(svmx)\n","\n","print(classification_report(ytr, preds))\n","print(confusion_matrix(ytr, preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-15T19:23:01.055954Z","iopub.status.idle":"2023-04-15T19:23:01.056824Z","shell.execute_reply":"2023-04-15T19:23:01.056579Z","shell.execute_reply.started":"2023-04-15T19:23:01.056551Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","with open('svm.pkl', 'wb') as f:\n","    pickle.dump(clf, f)"]},{"cell_type":"markdown","metadata":{},"source":["# All below cells are results of various ensembling strats and baselines\n"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T09:32:32.076671Z","iopub.status.busy":"2023-04-16T09:32:32.076226Z","iopub.status.idle":"2023-04-16T09:32:32.113190Z","shell.execute_reply":"2023-04-16T09:32:32.112177Z","shell.execute_reply.started":"2023-04-16T09:32:32.076629Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["m1 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","m2 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","s1 = torch.load('/kaggle/working/raw.pt')\n","s2 = torch.load('/kaggle/working/ss.pt')\n","m1.load_state_dict(s1)\n","m2.load_state_dict(s2)"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T09:56:42.686085Z","iopub.status.busy":"2023-04-16T09:56:42.685639Z","iopub.status.idle":"2023-04-16T09:56:52.477339Z","shell.execute_reply":"2023-04-16T09:56:52.476165Z","shell.execute_reply.started":"2023-04-16T09:56:42.686050Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([9645, 128])\n","torch.Size([2412, 128])\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.98      0.95      5041\n","         1.0       0.97      0.90      0.94      4604\n","\n","    accuracy                           0.94      9645\n","   macro avg       0.94      0.94      0.94      9645\n","weighted avg       0.94      0.94      0.94      9645\n","\n","[[4915  126]\n"," [ 446 4158]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.85      0.91      0.88      1278\n","         1.0       0.89      0.82      0.85      1134\n","\n","    accuracy                           0.87      2412\n","   macro avg       0.87      0.87      0.87      2412\n","weighted avg       0.87      0.87      0.87      2412\n","\n","[[1168  110]\n"," [ 208  926]]\n"]}],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","svmx_train = []\n","svmy_train = []\n","svmx_test = []\n","svmy_test = []\n","\n","ytr = []\n","cury = y_train.detach().cpu().numpy()\n","xtr1 = []\n","xtr2 = []\n","yte = []\n","curyte = y_test.detach().cpu().numpy()\n","xte1 = []\n","xte2 = []\n","for i in range(len(cury)):\n","    ytr.append(cury[i])\n","    xtr1.append(X_train_claim[i].detach().cpu().numpy())\n","    xtr2.append(X_train_ss[i].detach().cpu().numpy())\n","for i in range(len(curyte)):\n","    yte.append(curyte[i])\n","    xte1.append(X_test_claim[i].detach().cpu().numpy())\n","    xte2.append(X_test_ss[i].detach().cpu().numpy())\n","\n","\n","activation = {}\n","def get_activation(name):\n","    def hook(model, input, output):\n","        activation[name] = output\n","    return hook\n","\n","m1.bilstm2.register_forward_hook(get_activation('bilstm2_m1'))\n","m1.eval()\n","m2.bilstm2.register_forward_hook(get_activation('bilstm2_m2'))\n","m2.eval()\n","with torch.no_grad():\n","    weighted_avg = (torch.tensor(0.8) * activation['bilstm2_m1'][0][:, -1, :].detach().cpu()) + (torch.tensor(0.2) * activation['bilstm2_m2'][0][:, -1, :].detach().cpu())\n","    svmx_train = weighted_avg.numpy().tolist()\n","    svmy_train = ytr\n","    \n","    weighted_avg = (torch.tensor(0.8) * activation['bilstm2_m1'][0][:, -1, :].detach().cpu()) + (torch.tensor(0.2) * activation['bilstm2_m2'][0][:, -1, :].detach().cpu())\n","    svmx_test = weighted_avg.numpy().tolist()\n","    svmy_test = yte\n","    \n","\n","clf = svm.SVC(probability=True, kernel='poly')\n","clf.fit(svmx_train, svmy_train)\n","preds = clf.predict(svmx_train)\n","print(classification_report(svmy_train, preds))\n","print(confusion_matrix(svmy_train, preds))\n","\n","\n","preds = clf.predict(svmx_test)\n","print(classification_report(svmy_test, preds))\n","print(confusion_matrix(svmy_test, preds))"]},{"cell_type":"markdown","metadata":{},"source":["# SVM\n","            precision    recall  f1-score   support\n","\n","         0.0       0.78      0.87      0.82      5041\n","         1.0       0.84      0.72      0.78      4604\n","\n","    accuracy                           0.80      9645\n","    macro avg       0.81      0.80      0.80      9645\n","    weighted avg       0.80      0.80      0.80      9645\n","\n","[[4388  653]\n"," [1272 3332]]\n","              \n","              precision    recall  f1-score   support\n","\n","         0.0       0.77      0.87      0.82      1278\n","         1.0       0.83      0.71      0.77      1134\n","\n","    accuracy                           0.80      2412\n","    macro avg       0.80      0.79      0.79      2412\n","    weighted avg       0.80      0.80      0.79      2412\n","\n","[[1113  165]\n"," [ 328  806]]\n"," \n","# Best BiLSTM\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.95      0.94      5041\n","         1.0       0.94      0.92      0.93      4604\n","\n","    accuracy                           0.93      9645\n","    macro avg       0.94      0.93      0.93      9645\n","    weighted avg       0.94      0.93      0.93      9645\n","\n","[[4775  266]\n"," [ 361 4243]]\n","              \n","              precision    recall  f1-score   support\n","\n","         0.0       0.87      0.88      0.87      1278\n","         1.0       0.86      0.85      0.85      1134\n","\n","    accuracy                           0.86      2412\n","    macro avg       0.86      0.86      0.86      2412\n","    weighted avg       0.86      0.86      0.86      2412\n","\n","[[1125  153]\n"," [ 174  960]]"]},{"cell_type":"markdown","metadata":{},"source":["## Dense layer to calculate the weights in weighted average"]},{"cell_type":"code","execution_count":153,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T11:09:11.553841Z","iopub.status.busy":"2023-04-16T11:09:11.552715Z","iopub.status.idle":"2023-04-16T11:09:21.724409Z","shell.execute_reply":"2023-04-16T11:09:21.722875Z","shell.execute_reply.started":"2023-04-16T11:09:11.553773Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting hummingbird-ml\n","  Using cached hummingbird_ml-0.4.4-py2.py3-none-any.whl (181 kB)\n","Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.7/site-packages (from hummingbird-ml) (1.21.6)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from hummingbird-ml) (1.7.3)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from hummingbird-ml) (1.0.2)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from hummingbird-ml) (0.3.6)\n","Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from hummingbird-ml) (1.13.0)\n","Collecting onnxconverter-common>=1.6.0\n","  Using cached onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from hummingbird-ml) (5.9.3)\n","Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from onnxconverter-common>=1.6.0->hummingbird-ml) (3.20.3)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from onnxconverter-common>=1.6.0->hummingbird-ml) (23.0)\n","Requirement already satisfied: onnx in /opt/conda/lib/python3.7/site-packages (from onnxconverter-common>=1.6.0->hummingbird-ml) (1.13.1)\n","Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->hummingbird-ml) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->hummingbird-ml) (3.1.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->hummingbird-ml) (4.4.0)\n","Installing collected packages: onnxconverter-common, hummingbird-ml\n","Successfully installed hummingbird-ml-0.4.4 onnxconverter-common-1.13.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install hummingbird-ml"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T10:01:00.484925Z","iopub.status.busy":"2023-04-16T10:01:00.483889Z","iopub.status.idle":"2023-04-16T10:01:00.515581Z","shell.execute_reply":"2023-04-16T10:01:00.514481Z","shell.execute_reply.started":"2023-04-16T10:01:00.484880Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["m1 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","m2 = BiLSTM2(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n","s1 = torch.load('/kaggle/working/raw.pt')\n","s2 = torch.load('/kaggle/working/ss.pt')\n","m1.load_state_dict(s1)\n","m2.load_state_dict(s2)"]},{"cell_type":"code","execution_count":117,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T10:26:04.371953Z","iopub.status.busy":"2023-04-16T10:26:04.371239Z","iopub.status.idle":"2023-04-16T10:26:04.383241Z","shell.execute_reply":"2023-04-16T10:26:04.381942Z","shell.execute_reply.started":"2023-04-16T10:26:04.371915Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(tensor(0.0007, grad_fn=<MseLossBackward0>),\n"," tensor([0.3155], requires_grad=True),\n"," tensor([0.3427], requires_grad=True))"]},"execution_count":117,"metadata":{},"output_type":"execute_result"}],"source":["a = torch.rand((1), requires_grad=True)\n","b = torch.rand((1), requires_grad=True)\n","c = nn.MSELoss()\n","loss = c(a, b)\n","loss, a, b"]},{"cell_type":"code","execution_count":167,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T11:29:28.101764Z","iopub.status.busy":"2023-04-16T11:29:28.101325Z","iopub.status.idle":"2023-04-16T11:29:36.152001Z","shell.execute_reply":"2023-04-16T11:29:36.150363Z","shell.execute_reply.started":"2023-04-16T11:29:28.101727Z"},"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/656798959.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvmx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mwloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvmy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mwloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."]}],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","from hummingbird.ml import convert, load\n","\n","class AccuracyLoss(nn.Module):\n","    def __init__(self):\n","        super(AccuracyLoss, self).__init__()\n","\n","    def forward(self, y_true, y_preds):\n","        loes = torch.sum(y_true == y_preds) / torch.tensor(len(y_preds)).to(device)\n","        return loss\n","\n","w1 = torch.rand((1)).to(device)\n","w2 = torch.rand((1)).to(device)\n","# weighter_model = Weighter(2, 1)\n","criterion = AccuracyLoss()\n","optimizer = optim.SGD([w1, w2], lr=0.01)\n","\n","ytr = []\n","cury = y_train.detach().cpu().numpy()\n","xtr1 = []\n","xtr2 = []\n","yte = []\n","curyte = y_test.detach().cpu().numpy()\n","xte1 = []\n","xte2 = []\n","for i in range(len(cury)):\n","    ytr.append(cury[i])\n","    xtr1.append(X_train_claim[i].detach().cpu().numpy())\n","    xtr2.append(X_train_ss[i].detach().cpu().numpy())\n","for i in range(len(curyte)):\n","    yte.append(curyte[i])\n","    xte1.append(X_test_claim[i].detach().cpu().numpy())\n","    xte2.append(X_test_ss[i].detach().cpu().numpy())\n","\n","\n","activation = {}\n","def get_activation(name):\n","    def hook(model, input, output):\n","        activation[name] = output\n","    return hook\n","\n","m1.bilstm2.register_forward_hook(get_activation('bilstm2_m1'))\n","m1.eval()\n","m2.bilstm2.register_forward_hook(get_activation('bilstm2_m2'))\n","m2.eval()\n","wepochs = 3\n","\n","\n","def compute_xy():\n","    p1 = m1(torch.tensor(np.array(xtr1)).to(device))\n","    p1 = (p1 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","    p2 = m2(torch.tensor(np.array(xtr2)).to(device))\n","    p2 = (p2 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","    weighted_avg = (w1 * activation['bilstm2_m1'][0][:, -1, :].clone().detach()) + (w2 * activation['bilstm2_m2'][0][:, -1, :].clone().detach())\n","    svmx_train = weighted_avg\n","    svmy_train = ytr\n","\n","    p1 = m1(torch.tensor(np.array(xte1)).to(device))\n","    p1 = (p1 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","    p2 = m2(torch.tensor(np.array(xte2)).to(device))\n","    p2 = (p2 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n","    weighted_avg = (w1 * activation['bilstm2_m1'][0][:, -1, :].clone().detach()) + (w2* activation['bilstm2_m2'][0][:, -1, :].clone().detach())\n","    svmx_test = weighted_avg\n","    svmy_test = yte\n","    return svmx_train, torch.tensor(svmy_train).to(device), svmx_test, torch.tensor(svmy_test).to(device)\n","\n","\n","# with torch.no_grad():\n","for epoch in range(wepochs):\n","    optimizer.zero_grad()\n","    \n","    svmx_train, svmy_train, svmx_test, svmy_test = compute_xy()\n","    \n","    clf = svm.SVC(probability=True, kernel='poly')\n","    clf.fit(svmx_train.clone().detach().cpu().numpy(), svmy_train.cpu().numpy())\n","    preds = torch.tensor(clf.predict(svmx_train.clone().detach().cpu().numpy())).to(device)\n","    wloss = criterion(svmy_train, preds)\n","    wloss.backward()\n","    print(wloss)\n","    optimizer.step()\n","    print(w1, w2)\n"]},{"cell_type":"code","execution_count":163,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T11:20:52.580900Z","iopub.status.busy":"2023-04-16T11:20:52.580479Z","iopub.status.idle":"2023-04-16T11:20:52.698222Z","shell.execute_reply":"2023-04-16T11:20:52.696896Z","shell.execute_reply.started":"2023-04-16T11:20:52.580857Z"},"trusted":true},"outputs":[{"data":{"image/svg+xml":["<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n","<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n"," \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n","<!-- Generated by graphviz version 7.1.0 (20230122.1345)\n"," -->\n","<!-- Pages: 1 -->\n","<svg width=\"228pt\" height=\"326pt\"\n"," viewBox=\"0.00 0.00 228.00 326.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n","<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 322)\">\n","<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-322 224,-322 224,4 -4,4\"/>\n","<!-- 138098404476400 -->\n","<g id=\"node1\" class=\"node\">\n","<title>138098404476400</title>\n","<polygon fill=\"#caff70\" stroke=\"black\" points=\"136.5,-31 82.5,-31 82.5,0 136.5,0 136.5,-31\"/>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n","</g>\n","<!-- 138098301045328 -->\n","<g id=\"node2\" class=\"node\">\n","<title>138098301045328</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"157,-86 62,-86 62,-67 157,-67 157,-86\"/>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n","</g>\n","<!-- 138098301045328&#45;&gt;138098404476400 -->\n","<g id=\"edge8\" class=\"edge\">\n","<title>138098301045328&#45;&gt;138098404476400</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M109.5,-66.54C109.5,-60.07 109.5,-50.98 109.5,-42.32\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"113,-42.58 109.5,-32.58 106,-42.58 113,-42.58\"/>\n","</g>\n","<!-- 138098302166864 -->\n","<g id=\"node3\" class=\"node\">\n","<title>138098302166864</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"154,-141 65,-141 65,-122 154,-122 154,-141\"/>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n","</g>\n","<!-- 138098302166864&#45;&gt;138098301045328 -->\n","<g id=\"edge1\" class=\"edge\">\n","<title>138098302166864&#45;&gt;138098301045328</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M109.5,-121.75C109.5,-115.27 109.5,-106.16 109.5,-97.9\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"113,-97.96 109.5,-87.96 106,-97.96 113,-97.96\"/>\n","</g>\n","<!-- 138098302371472 -->\n","<g id=\"node4\" class=\"node\">\n","<title>138098302371472</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 12,-196 12,-177 101,-177 101,-196\"/>\n","<text text-anchor=\"middle\" x=\"56.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n","</g>\n","<!-- 138098302371472&#45;&gt;138098302166864 -->\n","<g id=\"edge2\" class=\"edge\">\n","<title>138098302371472&#45;&gt;138098302166864</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M65.25,-176.75C72.65,-169.35 83.46,-158.54 92.55,-149.45\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"94.89,-152.06 99.49,-142.51 89.94,-147.11 94.89,-152.06\"/>\n","</g>\n","<!-- 138098301043024 -->\n","<g id=\"node5\" class=\"node\">\n","<title>138098301043024</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-251 0,-251 0,-232 101,-232 101,-251\"/>\n","<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n","</g>\n","<!-- 138098301043024&#45;&gt;138098302371472 -->\n","<g id=\"edge3\" class=\"edge\">\n","<title>138098301043024&#45;&gt;138098302371472</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M51.49,-231.75C52.23,-225.19 53.28,-215.95 54.22,-207.6\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"57.67,-208.28 55.32,-197.95 50.71,-207.49 57.67,-208.28\"/>\n","</g>\n","<!-- 138098426441168 -->\n","<g id=\"node6\" class=\"node\">\n","<title>138098426441168</title>\n","<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-318 23.5,-318 23.5,-287 77.5,-287 77.5,-318\"/>\n","<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n","</g>\n","<!-- 138098426441168&#45;&gt;138098301043024 -->\n","<g id=\"edge4\" class=\"edge\">\n","<title>138098426441168&#45;&gt;138098301043024</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M50.5,-286.61C50.5,-279.35 50.5,-270.51 50.5,-262.66\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"54,-262.67 50.5,-252.67 47,-262.67 54,-262.67\"/>\n","</g>\n","<!-- 138098302373072 -->\n","<g id=\"node7\" class=\"node\">\n","<title>138098302373072</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"211,-196 122,-196 122,-177 211,-177 211,-196\"/>\n","<text text-anchor=\"middle\" x=\"166.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n","</g>\n","<!-- 138098302373072&#45;&gt;138098302166864 -->\n","<g id=\"edge5\" class=\"edge\">\n","<title>138098302373072&#45;&gt;138098302166864</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M156.83,-176.51C148.76,-169 137.03,-158.1 127.28,-149.03\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"129.84,-146.63 120.13,-142.39 125.07,-151.76 129.84,-146.63\"/>\n","</g>\n","<!-- 138098299721488 -->\n","<g id=\"node8\" class=\"node\">\n","<title>138098299721488</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"220,-251 119,-251 119,-232 220,-232 220,-251\"/>\n","<text text-anchor=\"middle\" x=\"169.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n","</g>\n","<!-- 138098299721488&#45;&gt;138098302373072 -->\n","<g id=\"edge6\" class=\"edge\">\n","<title>138098299721488&#45;&gt;138098302373072</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M169,-231.75C168.64,-225.27 168.12,-216.16 167.65,-207.9\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"171.15,-207.74 167.09,-197.95 164.16,-208.13 171.15,-207.74\"/>\n","</g>\n","<!-- 138098397880784 -->\n","<g id=\"node9\" class=\"node\">\n","<title>138098397880784</title>\n","<polygon fill=\"lightblue\" stroke=\"black\" points=\"196.5,-318 142.5,-318 142.5,-287 196.5,-287 196.5,-318\"/>\n","<text text-anchor=\"middle\" x=\"169.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n","</g>\n","<!-- 138098397880784&#45;&gt;138098299721488 -->\n","<g id=\"edge7\" class=\"edge\">\n","<title>138098397880784&#45;&gt;138098299721488</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M169.5,-286.61C169.5,-279.35 169.5,-270.51 169.5,-262.66\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"173,-262.67 169.5,-252.67 166,-262.67 173,-262.67\"/>\n","</g>\n","</g>\n","</svg>\n"],"text/plain":["<graphviz.dot.Digraph at 0x7d99842d1e50>"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["from torchviz import make_dot\n","\n","make_dot(wloss, params={'w1': w1, 'w2': w2})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","\n","# define initial values of a and b\n","a = torch.tensor(1.0, requires_grad=True)\n","b = torch.tensor(1.0, requires_grad=True)\n","\n","# define your loss function that takes a and b as input and returns a scalar value\n","def loss_function(a, b):\n","    loss = (a**2 + b**2).mean()  # example loss function, replace with your own\n","    print(loss)\n","    return loss\n","\n","# create an optimizer and set its learning rate\n","optimizer = optim.SGD([a, b], lr=0.1)  # stochastic gradient descent optimizer\n","\n","# training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    # compute the loss value\n","    loss = loss_function(a, b)\n","    \n","    # compute gradients of the loss with respect to a and b\n","    loss.backward()\n","    \n","    # update the values of a and b using the optimizer\n","    optimizer.step()\n","    \n","    # zero the gradients of a and b for the next iteration\n","    optimizer.zero_grad()\n","    \n","    # print the loss value every 100 epochs\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch}, Loss: {loss.item()},\", a.item(), b.item())\n","\n","# print the final values of a and b\n","print(f\"Final values of a and b: {a.item()}, {b.item()}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Baselines"]},{"cell_type":"code","execution_count":170,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T11:49:54.441221Z","iopub.status.busy":"2023-04-16T11:49:54.440853Z","iopub.status.idle":"2023-04-16T11:49:54.470631Z","shell.execute_reply":"2023-04-16T11:49:54.469661Z","shell.execute_reply.started":"2023-04-16T11:49:54.441188Z"},"trusted":true},"outputs":[],"source":["baseline_x_train = X_train_claim.detach().cpu().numpy().tolist()\n","baseline_y_train = y_train.detach().cpu().numpy().tolist()\n","baseline_x_test = X_test_claim.detach().cpu().numpy().tolist()\n","baseline_y_test = y_test.detach().cpu().numpy().tolist()"]},{"cell_type":"code","execution_count":172,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T11:50:49.933766Z","iopub.status.busy":"2023-04-16T11:50:49.932741Z","iopub.status.idle":"2023-04-16T11:50:50.510256Z","shell.execute_reply":"2023-04-16T11:50:50.509158Z","shell.execute_reply.started":"2023-04-16T11:50:49.933711Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.76      0.86      0.81      5041\n","         1.0       0.82      0.70      0.76      4604\n","\n","    accuracy                           0.78      9645\n","   macro avg       0.79      0.78      0.78      9645\n","weighted avg       0.79      0.78      0.78      9645\n","\n","[[4335  706]\n"," [1382 3222]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.75      0.86      0.80      1278\n","         1.0       0.81      0.68      0.74      1134\n","\n","    accuracy                           0.77      2412\n","   macro avg       0.78      0.77      0.77      2412\n","weighted avg       0.78      0.77      0.77      2412\n","\n","[[1101  177]\n"," [ 368  766]]\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from tqdm import tqdm\n","from sklearn.svm import SVC\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, GRU\n","from keras.layers.core import Dense, Activation, Dropout\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.layers import BatchNormalization\n","from keras.utils import np_utils\n","from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n","from tensorflow.keras.preprocessing import sequence, text\n","\n","#Logistic Regression with TF-IDF\n","clf = LogisticRegression(C=1.0)\n","clf.fit(baseline_x_train, baseline_y_train)\n","# predictions = clf.predict_proba(baseline_x_test)\n","predictions = clf.predict(baseline_x_train)\n","# predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]\n","print(classification_report(baseline_y_train, predictions))\n","print(confusion_matrix(baseline_y_train, predictions))\n","\n","predictions = clf.predict(baseline_x_test)\n","print(classification_report(baseline_y_test, predictions))\n","print(confusion_matrix(baseline_y_test, predictions))"]},{"cell_type":"code","execution_count":173,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T11:52:57.041693Z","iopub.status.busy":"2023-04-16T11:52:57.041071Z","iopub.status.idle":"2023-04-16T11:52:57.271542Z","shell.execute_reply":"2023-04-16T11:52:57.270469Z","shell.execute_reply.started":"2023-04-16T11:52:57.041656Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.72      0.92      0.81      5041\n","         1.0       0.88      0.61      0.72      4604\n","\n","    accuracy                           0.77      9645\n","   macro avg       0.80      0.76      0.76      9645\n","weighted avg       0.79      0.77      0.76      9645\n","\n","[[4646  395]\n"," [1816 2788]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.71      0.92      0.80      1278\n","         1.0       0.87      0.58      0.70      1134\n","\n","    accuracy                           0.76      2412\n","   macro avg       0.79      0.75      0.75      2412\n","weighted avg       0.79      0.76      0.75      2412\n","\n","[[1180   98]\n"," [ 474  660]]\n"]}],"source":["#Naive Bayes\n","clf = MultinomialNB()\n","clf.fit(baseline_x_train, baseline_y_train)\n","predictions = clf.predict(baseline_x_train)\n","# predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]\n","print(classification_report(baseline_y_train, predictions))\n","print(confusion_matrix(baseline_y_train, predictions))\n","\n","predictions = clf.predict(baseline_x_test)\n","print(classification_report(baseline_y_test, predictions))\n","print(confusion_matrix(baseline_y_test, predictions))"]},{"cell_type":"code","execution_count":174,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T11:53:02.298443Z","iopub.status.busy":"2023-04-16T11:53:02.297824Z","iopub.status.idle":"2023-04-16T11:53:03.287811Z","shell.execute_reply":"2023-04-16T11:53:03.286836Z","shell.execute_reply.started":"2023-04-16T11:53:02.298387Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.79      0.87      0.83      5041\n","         1.0       0.84      0.74      0.79      4604\n","\n","    accuracy                           0.81      9645\n","   macro avg       0.81      0.81      0.81      9645\n","weighted avg       0.81      0.81      0.81      9645\n","\n","[[4401  640]\n"," [1199 3405]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.78      0.87      0.82      1278\n","         1.0       0.84      0.72      0.78      1134\n","\n","    accuracy                           0.80      2412\n","   macro avg       0.81      0.80      0.80      2412\n","weighted avg       0.81      0.80      0.80      2412\n","\n","[[1117  161]\n"," [ 313  821]]\n"]}],"source":["#XG-Boost classifier\n","clf = xgb.XGBClassifier(max_depth=3, n_estimators=50, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.01)\n","\n","clf.fit(baseline_x_train, baseline_y_train)\n","predictions = clf.predict(baseline_x_train)\n","# predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]\n","print(classification_report(baseline_y_train, predictions))\n","print(confusion_matrix(baseline_y_train, predictions))\n","\n","predictions = clf.predict(baseline_x_test)\n","print(classification_report(baseline_y_test, predictions))\n","print(confusion_matrix(baseline_y_test, predictions))"]},{"cell_type":"code","execution_count":176,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T11:53:19.801780Z","iopub.status.busy":"2023-04-16T11:53:19.801210Z","iopub.status.idle":"2023-04-16T11:53:20.407759Z","shell.execute_reply":"2023-04-16T11:53:20.406655Z","shell.execute_reply.started":"2023-04-16T11:53:19.801742Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.75      0.90      0.82      5041\n","         1.0       0.86      0.67      0.75      4604\n","\n","    accuracy                           0.79      9645\n","   macro avg       0.80      0.78      0.78      9645\n","weighted avg       0.80      0.79      0.79      9645\n","\n","[[4521  520]\n"," [1512 3092]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.75      0.90      0.82      1278\n","         1.0       0.86      0.66      0.75      1134\n","\n","    accuracy                           0.79      2412\n","   macro avg       0.80      0.78      0.78      2412\n","weighted avg       0.80      0.79      0.79      2412\n","\n","[[1151  127]\n"," [ 380  754]]\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","clf.fit(baseline_x_train, baseline_y_train)\n","predictions = clf.predict(baseline_x_train)\n","# predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]\n","print(classification_report(baseline_y_train, predictions))\n","print(confusion_matrix(baseline_y_train, predictions))\n","\n","predictions = clf.predict(baseline_x_test)\n","print(classification_report(baseline_y_test, predictions))\n","print(confusion_matrix(baseline_y_test, predictions))"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T09:47:26.574738Z","iopub.status.busy":"2023-04-16T09:47:26.574143Z","iopub.status.idle":"2023-04-16T09:48:01.030222Z","shell.execute_reply":"2023-04-16T09:48:01.028089Z","shell.execute_reply.started":"2023-04-16T09:47:26.574700Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.78      0.87      0.82      5041\n","         1.0       0.84      0.72      0.78      4604\n","\n","    accuracy                           0.80      9645\n","   macro avg       0.81      0.80      0.80      9645\n","weighted avg       0.80      0.80      0.80      9645\n","\n","[[4388  653]\n"," [1272 3332]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.77      0.87      0.82      1278\n","         1.0       0.83      0.71      0.77      1134\n","\n","    accuracy                           0.80      2412\n","   macro avg       0.80      0.79      0.79      2412\n","weighted avg       0.80      0.80      0.79      2412\n","\n","[[1113  165]\n"," [ 328  806]]\n"]}],"source":["clf = SVC(probability=True)\n","clf.fit(baseline_x_train, baseline_y_train)\n","predictions = clf.predict_proba(baseline_x_train)\n","predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]\n","print(classification_report(baseline_y_train, predictions))\n","print(confusion_matrix(baseline_y_train, predictions))\n","predictions = clf.predict_proba(baseline_x_test)\n","predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]\n","print(classification_report(baseline_y_test, predictions))\n","print(confusion_matrix(baseline_y_test, predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
