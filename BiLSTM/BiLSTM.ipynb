{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../scraper/datasets/latest.csv')\n",
    "le = LabelEncoder()\n",
    "cdf = df.copy()\n",
    "cdf['truth_value'] = le.fit_transform(cdf['truth_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karun\\AppData\\Local\\Temp\\ipykernel_21744\\2597134784.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n",
      "C:\\Users\\karun\\AppData\\Local\\Temp\\ipykernel_21744\\2597134784.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>claim</th>\n",
       "      <th>truth_value</th>\n",
       "      <th>source</th>\n",
       "      <th>simple_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ukraine theft homicide levels rose due power o...</td>\n",
       "      <td>0</td>\n",
       "      <td>vox-ukraine</td>\n",
       "      <td>ukraine theft homicide levels rose due power o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ukrainians beat two berlin residents speaking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>vox-ukraine</td>\n",
       "      <td>ukrainians beat two berlin residents speaking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>quote paul goebbels banderites</td>\n",
       "      <td>0</td>\n",
       "      <td>vox-ukraine</td>\n",
       "      <td>quote paul goebbels banderites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>culture good neighborliness course ukrainian s...</td>\n",
       "      <td>0</td>\n",
       "      <td>vox-ukraine</td>\n",
       "      <td>culture good neighborliness course ukrainian s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>us research ukraine led increase incidence tic...</td>\n",
       "      <td>0</td>\n",
       "      <td>vox-ukraine</td>\n",
       "      <td>us research ukraine led increase incidence tic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>chile law rights mutants genetically modified ...</td>\n",
       "      <td>0</td>\n",
       "      <td>vox-ukraine</td>\n",
       "      <td>chile law rights mutants approved ostap stakhi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>covid incidence rate became zero late may</td>\n",
       "      <td>0</td>\n",
       "      <td>vox-ukraine</td>\n",
       "      <td>covid incidence rate became zero late may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>risk death among children vaccinated covid tim...</td>\n",
       "      <td>0</td>\n",
       "      <td>vox-ukraine</td>\n",
       "      <td>risk death among children vaccinated covid tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>russias army destroyed</td>\n",
       "      <td>0</td>\n",
       "      <td>politifact</td>\n",
       "      <td>russias army destroyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>war ukraine</td>\n",
       "      <td>0</td>\n",
       "      <td>politifact</td>\n",
       "      <td>war ukraine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              claim  truth_value  \\\n",
       "0           0  ukraine theft homicide levels rose due power o...            0   \n",
       "1           1  ukrainians beat two berlin residents speaking ...            0   \n",
       "2           2                     quote paul goebbels banderites            0   \n",
       "3           3  culture good neighborliness course ukrainian s...            0   \n",
       "4           4  us research ukraine led increase incidence tic...            0   \n",
       "5           5  chile law rights mutants genetically modified ...            0   \n",
       "6           6          covid incidence rate became zero late may            0   \n",
       "7           7  risk death among children vaccinated covid tim...            0   \n",
       "8           8                             russias army destroyed            0   \n",
       "9           9                                        war ukraine            0   \n",
       "\n",
       "        source                                    simple_sentence  \n",
       "0  vox-ukraine  ukraine theft homicide levels rose due power o...  \n",
       "1  vox-ukraine  ukrainians beat two berlin residents speaking ...  \n",
       "2  vox-ukraine                     quote paul goebbels banderites  \n",
       "3  vox-ukraine  culture good neighborliness course ukrainian s...  \n",
       "4  vox-ukraine  us research ukraine led increase incidence tic...  \n",
       "5  vox-ukraine  chile law rights mutants approved ostap stakhi...  \n",
       "6  vox-ukraine          covid incidence rate became zero late may  \n",
       "7  vox-ukraine  risk death among children vaccinated covid tim...  \n",
       "8   politifact                             russias army destroyed  \n",
       "9   politifact                                        war ukraine  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemm_text(text):\n",
    "    return ' '.join([stemmer.stem(w) for w in text.split(' ')])\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])\n",
    "\n",
    "T = cdf['claim'].str.split(' \\n\\n---\\n\\n').str[0]\n",
    "T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n",
    "stop = stopwords.words('english')\n",
    "T = T.apply(lambda x: ' '.join([y for y in x.split() if not y.isdigit()]))\n",
    "T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
    "cdf['claim'] = T\n",
    "\n",
    "T = cdf['simple_sentence'].str.split(' \\n\\n---\\n\\n').str[0]\n",
    "T = T.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n",
    "stop = stopwords.words('english')\n",
    "T = T.apply(lambda x: ' '.join([y for y in x.split() if not y.isdigit()]))\n",
    "T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
    "cdf['simple_sentence'] = T\n",
    "cdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     certificate has expired (_ssl.c:997)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0                                              claim  truth_value  \\\n",
       " 0         409  missiles launched american forces iraq iran fu...            0   \n",
       " 1         303  ukrainians bled died congressman budd excused ...            0   \n",
       " 2         514  state union address democrats even positive ne...            0   \n",
       " 3         413  says democratic leadership presidential candid...            0   \n",
       " 4         165  says president donald trumps hold ukraine aid ...            0   \n",
       " \n",
       "        source                                    simple_sentence  \n",
       " 0  politifact  missiles launched american forces iraq iran fu...  \n",
       " 1  politifact  ukrainians bled congressman budd excused kille...  \n",
       " 2  politifact  state union address democrats even positive ne...  \n",
       " 3  politifact  says democratic leadership mourning loss qasse...  \n",
       " 4  politifact  says president donald trump hold ukraine aid l...  ,\n",
       " 0    566\n",
       " 1    566\n",
       " Name: truth_value, dtype: int64)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf = cdf.copy()\n",
    "grouped = adf.groupby('truth_value')\n",
    "\n",
    "# Sample 900 rows from each group\n",
    "sampled = grouped.apply(lambda x: x.sample(n=566))\n",
    "\n",
    "# Reset the index of the sampled data\n",
    "sampled = sampled.reset_index(drop=True)\n",
    "sampled.head(), sampled['truth_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karun\\AppData\\Local\\Temp\\ipykernel_21744\\1130097206.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['claim'] = X['claim'].apply(lambda w: lemmatize_text(w))\n",
      "C:\\Users\\karun\\AppData\\Local\\Temp\\ipykernel_21744\\1130097206.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['simple_sentence'] = X['simple_sentence'].apply(lambda w: lemmatize_text(w))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>simple_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>missile launched american force iraq iran fund...</td>\n",
       "      <td>missile launched american force iraq iran fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ukrainian bled died congressman budd excused k...</td>\n",
       "      <td>ukrainian bled congressman budd excused killer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>state union address democrat even positive new...</td>\n",
       "      <td>state union address democrat even positive new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>say democratic leadership presidential candida...</td>\n",
       "      <td>say democratic leadership mourning loss qassem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>say president donald trump hold ukraine aid li...</td>\n",
       "      <td>say president donald trump hold ukraine aid li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  missile launched american force iraq iran fund...   \n",
       "1  ukrainian bled died congressman budd excused k...   \n",
       "2  state union address democrat even positive new...   \n",
       "3  say democratic leadership presidential candida...   \n",
       "4  say president donald trump hold ukraine aid li...   \n",
       "\n",
       "                                     simple_sentence  \n",
       "0  missile launched american force iraq iran fund...  \n",
       "1  ukrainian bled congressman budd excused killer...  \n",
       "2  state union address democrat even positive new...  \n",
       "3  say democratic leadership mourning loss qassem...  \n",
       "4  say president donald trump hold ukraine aid li...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sampled[['claim', 'simple_sentence']]\n",
    "y = sampled['truth_value']\n",
    "\n",
    "X['claim'] = X['claim'].apply(lambda w: lemmatize_text(w))\n",
    "X['simple_sentence'] = X['simple_sentence'].apply(lambda w: lemmatize_text(w))\n",
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import sequence, text\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeAndGenerateSequences(X, y):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    tk1 = text.Tokenizer(num_words=2000)\n",
    "    tk2 = text.Tokenizer(num_words=2000)\n",
    "    tk1.fit_on_texts(xtrain['claim'])\n",
    "    tk2.fit_on_texts(xtrain['simple_sentence'])\n",
    "    tokenized_train_claim = tk1.texts_to_sequences(xtrain['claim'])\n",
    "    tokenized_train_ss = tk2.texts_to_sequences(xtrain['simple_sentence'])\n",
    "    X_train_claim = torch.tensor(tf.keras.preprocessing.sequence.pad_sequences(tokenized_train_claim, maxlen=60)).to(device)\n",
    "    X_train_ss = torch.tensor(tf.keras.preprocessing.sequence.pad_sequences(tokenized_train_ss, maxlen=60)).to(device)\n",
    "    tokenized_test_claim = tk1.texts_to_sequences(xtest['claim'])\n",
    "    tokenized_test_ss = tk1.texts_to_sequences(xtest['simple_sentence'])\n",
    "    X_test_claim = torch.tensor(tf.keras.preprocessing.sequence.pad_sequences(tokenized_test_claim, maxlen=60)).to(device)\n",
    "    X_test_ss = torch.tensor(tf.keras.preprocessing.sequence.pad_sequences(tokenized_test_ss, maxlen=60)).to(device)\n",
    "\n",
    "    # Convert labels to tensors\n",
    "    y_train = torch.tensor(ytrain.values).float().to(device)\n",
    "    y_test = torch.tensor(ytest.values).float().to(device)\n",
    "    \n",
    "    return X_train_claim, X_train_ss, y_train, X_test_claim, X_test_ss, y_test\n",
    "\n",
    "# X_train_txt, y_train_txt, X_test_txt, y_test_txt = xtrain, xtest, ytrain, ytest = train_test_split(cdf['claim'], cdf['truth_value'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeSentence(X, y, sentence):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    tk1 = text.Tokenizer(num_words=2000)\n",
    "    tk2 = text.Tokenizer(num_words=2000)\n",
    "    tk1.fit_on_texts(xtrain['claim'])\n",
    "    tk2.fit_on_texts(xtrain['simple_sentence'])\n",
    "    tokenized_train_claim = tk1.texts_to_sequences([sentence, ])\n",
    "    tokenized_train_ss = tk2.texts_to_sequences([simple_sentence, ])\n",
    "    X_train_claim = torch.tensor(tf.keras.preprocessing.sequence.pad_sequences(tokenized_train_claim, maxlen=60)).to(device)\n",
    "    X_train_ss = torch.tensor(tf.keras.preprocessing.sequence.pad_sequences(tokenized_train_ss, maxlen=60)).to(device)\n",
    "    y_train = torch.tensor(ytrain.values).float().to(device)\n",
    "\n",
    "    X_train_claim, X_train_ss, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_claim, X_train_ss, y_train, X_test_claim, X_test_ss, y_test = tokenizeAndGenerateSequences(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model architecture\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, num_words, embed_size, hidden_size, fc_out_size, output_size, dropout_rate):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_words, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.bilstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return self.sigmoid(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epochs: 1, Train Accuracy: 0.666077738515901 Test Accuracy: 0.6236749116607774\n",
      "Total Epochs: 2, Train Accuracy: 0.7040636042402827 Test Accuracy: 0.7137809187279152\n",
      "Total Epochs: 3, Train Accuracy: 0.7031802120141343 Test Accuracy: 0.6784452296819788\n",
      "Total Epochs: 4, Train Accuracy: 0.7093639575971732 Test Accuracy: 0.7084805653710248\n",
      "Total Epochs: 5, Train Accuracy: 0.7321554770318022 Test Accuracy: 0.7438162544169611\n",
      "Total Epochs: 6, Train Accuracy: 0.7638398115429919 Test Accuracy: 0.765017667844523\n",
      "Total Epochs: 7, Train Accuracy: 0.8066633013629481 Test Accuracy: 0.7526501766784452\n",
      "Total Epochs: 8, Train Accuracy: 0.7972614840989399 Test Accuracy: 0.7526501766784452\n",
      "Total Epochs: 9, Train Accuracy: 0.8168433451118964 Test Accuracy: 0.7473498233215548\n",
      "Total Epochs: 10, Train Accuracy: 0.800530035335689 Test Accuracy: 0.7579505300353356\n",
      "Total Epochs: 11, Train Accuracy: 0.8355284291680051 Test Accuracy: 0.7632508833922261\n",
      "Total Epochs: 12, Train Accuracy: 0.8596878680800942 Test Accuracy: 0.7473498233215548\n",
      "Total Epochs: 13, Train Accuracy: 0.8468333786354988 Test Accuracy: 0.7385159010600707\n",
      "Total Epochs: 14, Train Accuracy: 0.8406108026249369 Test Accuracy: 0.7296819787985865\n",
      "Total Epochs: 15, Train Accuracy: 0.8610129564193169 Test Accuracy: 0.7208480565371025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "te = 20\n",
    "acc = []\n",
    "tracc = []\n",
    "dataset = torch.utils.data.TensorDataset(X_train_ss, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1024, shuffle=True)\n",
    "curtraacc = []\n",
    "model = None\n",
    "for e in range(1, te+1):\n",
    "    ctracc = 0\n",
    "    model = BiLSTM(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(e):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_claim.to(device))\n",
    "        loss = criterion(outputs.squeeze(), y_train.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_train_claim.to(device))\n",
    "            predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n",
    "        train_accuracy = accuracy_score(y_train.to(device).to('cpu'), predictions)\n",
    "        ctracc += train_accuracy\n",
    "        curtraacc.append(train_accuracy)\n",
    "\n",
    "    ctracc /= e\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_ss)\n",
    "        predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n",
    "\n",
    "    print(f\"Total Epochs: {e}, Train Accuracy: {ctracc} Test Accuracy: {accuracy_score(y_test.to('cpu'), predictions)}\")\n",
    "    acc.append(accuracy_score(y_test.to('cpu'), predictions))\n",
    "    tracc.append(ctracc)\n",
    "print('Max acc -', max(acc), ' with epochs -', acc.index(max(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_new = X_test_claim.detach().cpu().numpy().tolist()\n",
    "y_test_new = y_test.detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = BiLSTM(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n",
    "m2 = BiLSTM(num_words=2000, embed_size=60, hidden_size=64, fc_out_size=5000, output_size=1, dropout_rate=0.2).to(device)\n",
    "s1 = torch.load(r'C:\\Users\\karun\\Documents\\Code\\FYP-Fake-News\\BiLSTM\\rs.pt', map_location=torch.device('cpu'))\n",
    "s2 = torch.load(r'C:\\Users\\karun\\Documents\\Code\\FYP-Fake-News\\BiLSTM\\ss.pt', map_location=torch.device('cpu'))\n",
    "m1.load_state_dict(s1)\n",
    "m2.load_state_dict(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.49      0.57       269\n",
      "         1.0       0.63      0.79      0.70       297\n",
      "\n",
      "    accuracy                           0.65       566\n",
      "   macro avg       0.66      0.64      0.64       566\n",
      "weighted avg       0.66      0.65      0.64       566\n",
      "\n",
      "[[132 137]\n",
      " [ 61 236]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.53      0.59       269\n",
      "         1.0       0.64      0.76      0.69       297\n",
      "\n",
      "    accuracy                           0.65       566\n",
      "   macro avg       0.65      0.64      0.64       566\n",
      "weighted avg       0.65      0.65      0.64       566\n",
      "\n",
      "[[142 127]\n",
      " [ 72 225]]\n"
     ]
    }
   ],
   "source": [
    "svmx = []\n",
    "svmy = []\n",
    "\n",
    "ytr = []\n",
    "cury = y_test.detach().cpu().numpy()\n",
    "xtr = []\n",
    "for i in range(len(cury)):\n",
    "    ytr.append(cury[i])\n",
    "    xtr.append(X_test_ss[i].detach().cpu().numpy())\n",
    "    \n",
    "m1.eval()\n",
    "m2.eval()\n",
    "with torch.no_grad():\n",
    "    p1 = m1(torch.tensor(np.array(xtr)).to(device))\n",
    "    p1 = (p1 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n",
    "    p2 = m2(torch.tensor(np.array(xtr)).to(device))\n",
    "    p2 = (p2 > 0.5).to('cpu').int().squeeze().numpy().tolist()\n",
    "    svmx = [X_test_ss[i].detach().cpu().numpy().tolist() + [p1[i], p2[i]] for i in range(len(p1))]\n",
    "    \n",
    "    Y_TEST = torch.tensor(np.array(ytr)) \n",
    "    \n",
    "\n",
    "    print(classification_report(Y_TEST, p1))\n",
    "    print(confusion_matrix(Y_TEST, p1))\n",
    "    print(classification_report(Y_TEST, p2))\n",
    "    print(confusion_matrix(Y_TEST, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(svmx, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.81      0.76       269\n",
      "         1.0       0.80      0.70      0.75       297\n",
      "\n",
      "    accuracy                           0.75       566\n",
      "   macro avg       0.76      0.76      0.75       566\n",
      "weighted avg       0.76      0.75      0.75       566\n",
      "\n",
      "[[218  51]\n",
      " [ 88 209]]\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(svmx)\n",
    "print(classification_report(ytr, preds))\n",
    "print(confusion_matrix(ytr, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a897ce92e780bd0a113d3cbc4b391fa94b12f7aeaa7a9fdb40b859980fef9d59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
