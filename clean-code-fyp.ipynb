{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk==3.8.1 sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:18.219534Z","iopub.execute_input":"2023-02-25T17:48:18.220025Z","iopub.status.idle":"2023-02-25T17:48:28.330881Z","shell.execute_reply.started":"2023-02-25T17:48:18.219989Z","shell.execute_reply":"2023-02-25T17:48:28.329675Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk==3.8.1 in /opt/conda/lib/python3.7/site-packages (3.8.1)\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.7/site-packages (2.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (4.64.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (8.1.3)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (2021.11.10)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk==3.8.1) (1.2.0)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.13.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.26.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.14.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, BatchNormalization, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D, Input, Dropout\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.preprocessing import sequence, text\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom pathlib import Path\nimport random\nfrom collections import defaultdict\nfrom itertools import chain, groupby\nfrom typing import Any, List, Optional, Union\nimport joblib\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.exceptions import NotFittedError\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import InputExample, SentenceTransformer, losses","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-25T17:48:28.333482Z","iopub.execute_input":"2023-02-25T17:48:28.334163Z","iopub.status.idle":"2023-02-25T17:48:28.350563Z","shell.execute_reply.started":"2023-02-25T17:48:28.334116Z","shell.execute_reply":"2023-02-25T17:48:28.347027Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def read_csv(name):\n    df = pd.read_csv(name)\n    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n    return df\n\ndef convert_truth_values(df):\n    df.loc[df['truth_value'] == 'tom_ruling_pof', 'truth_value'] = 'meter-false'\n    df.loc[df['truth_value'] == 'meter-half-true', 'truth_value'] = 'meter-true'\n    df.loc[df['truth_value'] == 'meter-mostly-true', 'truth_value'] = 'meter-true'\n    df.loc[df['truth_value'] == 'meter-mostly-false', 'truth_value'] = 'meter-false'\n    return df\n\ndef drop_na(df):\n    df = df.dropna(subset=['claim'])\n    return df\n\ndef label_encode(df, column_name):\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df\n\nstemmer = SnowballStemmer(\"english\")\ndef stemm_text(text):\n    return ' '.join([stemmer.stem(w) for w in text.split(' ')])\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_text(text):\n    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])\n\ndef replace_newlines(series):\n    return series.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n\ndef remove_digits(series):\n    return series.apply(lambda x: ' '.join([y for y in x.split() if not y.isdigit()]))\n\ndef remove_stopwords(series):\n    stop = stopwords.words('english')\n    return series.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n\ndef _train_test_split(X, y):\n    return train_test_split(\n        X, y, stratify=y, random_state=42, test_size=0.4, shuffle=True)\n\ndef count_vectorize(data):\n    count_vectorizer = CountVectorizer()\n    emb = count_vectorizer.fit_transform(data)\n    return emb, count_vectorizer\n\ndef tf_idf(data, max_features=None):\n    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n    train = tfidf_vectorizer.fit_transform(data)\n    return train, tfidf_vectorizer\n\ndef run_ml_model(classifier, X_train, y_train, X_test, y_test, classifier_name='ML Model'):\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict_proba(X_test)\n    predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]\n\n    accuracy = sklearn.metrics.accuracy_score(predictions, y_test)\n    print(f'{classifier_name} accuracy - {accuracy}')\n\ndef set_torch_seed(val=42):\n    torch.manual_seed(val)\n\ndef get_device():\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef tokenize_and_generate_sequences(X_train, y_train, X_test, y_test):\n    # Tokenize the training data\n    tokenizer = text.Tokenizer(num_words=1000)\n    tokenizer.fit_on_texts(X_train)\n    tokenized_train = tokenizer.texts_to_sequences(X_train)\n    X_train = torch.tensor(sequence.pad_sequences(tokenized_train, maxlen=60)).to(device)\n\n    # Tokenize the test data\n    tokenized_test = tokenizer.texts_to_sequences(X_test)\n    X_test = torch.tensor(sequence.pad_sequences(tokenized_test, maxlen=60)).to(device)\n\n    # Convert labels to tensors\n    y_train = torch.tensor(y_train.values).float().to(device)\n    y_test = torch.tensor(y_test.values).float().to(device)\n    \n    return X_train, y_train, X_test, y_test\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:28.352681Z","iopub.execute_input":"2023-02-25T17:48:28.353241Z","iopub.status.idle":"2023-02-25T17:48:28.377426Z","shell.execute_reply.started":"2023-02-25T17:48:28.353203Z","shell.execute_reply":"2023-02-25T17:48:28.376263Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df = read_csv('/kaggle/input/ru-ukr-onlyv4/ru-ukr-onlyv4.csv')\ndf = convert_truth_values(df)\ndf = drop_na(df)\ndf = label_encode(df, 'truth_value')\ndf['claim'] = replace_newlines(df['claim'])\ndf['claim'] = remove_digits(df['claim'])\ndf['claim'] = remove_stopwords(df['claim'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:28.383613Z","iopub.execute_input":"2023-02-25T17:48:28.384540Z","iopub.status.idle":"2023-02-25T17:48:28.545009Z","shell.execute_reply.started":"2023-02-25T17:48:28.384503Z","shell.execute_reply":"2023-02-25T17:48:28.544071Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: The default value of regex will change from True to False in a future version.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                               claim  truth_value\n0                provocation disinformation overview            0\n1  ukraine theft homicide levels rose due power o...            0\n2  ukrainians beat two berlin residents speaking ...            0\n3                     quote paul goebbels banderites            0\n4  culture good neighborliness course ukrainian s...            0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>claim</th>\n      <th>truth_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>provocation disinformation overview</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ukraine theft homicide levels rose due power o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ukrainians beat two berlin residents speaking ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>quote paul goebbels banderites</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>culture good neighborliness course ukrainian s...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = _train_test_split(df['claim'], df['truth_value'])\nx_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nx_test.reset_index(drop=True, inplace=True)\nx_test.reset_index(drop=True, inplace=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:28.546241Z","iopub.execute_input":"2023-02-25T17:48:28.546703Z","iopub.status.idle":"2023-02-25T17:48:28.560849Z","shell.execute_reply.started":"2023-02-25T17:48:28.546664Z","shell.execute_reply":"2023-02-25T17:48:28.559437Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((1609,), (1609,), (1074,), (1074,))"},"metadata":{}}]},{"cell_type":"code","source":"x_train_counts, count_vectorizer = count_vectorize(x_train)\nx_test_counts = count_vectorizer.transform(x_test)\nx_train_counts.shape, x_test_counts.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:28.562678Z","iopub.execute_input":"2023-02-25T17:48:28.563028Z","iopub.status.idle":"2023-02-25T17:48:28.607764Z","shell.execute_reply.started":"2023-02-25T17:48:28.562993Z","shell.execute_reply":"2023-02-25T17:48:28.606707Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"((1609, 4501), (1074, 4501))"},"metadata":{}}]},{"cell_type":"code","source":"x_train_tfidf, tfidf_vectorizer = tf_idf(x_train)\nx_test_tfidf = tfidf_vectorizer.transform(x_test)\nx_train_tfidf.shape, x_test_tfidf.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:28.609178Z","iopub.execute_input":"2023-02-25T17:48:28.609644Z","iopub.status.idle":"2023-02-25T17:48:28.656148Z","shell.execute_reply.started":"2023-02-25T17:48:28.609604Z","shell.execute_reply":"2023-02-25T17:48:28.655073Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"((1609, 4501), (1074, 4501))"},"metadata":{}}]},{"cell_type":"code","source":"#Check the maximum number of words in a sentence\n#Helps with padding\nx_train.apply(lambda x: len(str(x).split())).max()","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:28.657719Z","iopub.execute_input":"2023-02-25T17:48:28.658157Z","iopub.status.idle":"2023-02-25T17:48:28.667405Z","shell.execute_reply.started":"2023-02-25T17:48:28.658121Z","shell.execute_reply":"2023-02-25T17:48:28.666389Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"49"},"metadata":{}}]},{"cell_type":"markdown","source":"# Machine Learning methods","metadata":{}},{"cell_type":"code","source":"run_ml_model(LogisticRegression(C=1.0, multi_class='ovr', solver='liblinear'), x_train_counts, y_train, x_test_counts, y_test, \n             'LogisticRegression with Bag of words')\nrun_ml_model(LogisticRegression(C=1.0, multi_class='ovr', solver='liblinear'), x_train_tfidf, y_train, x_test_tfidf, y_test, \n             'LogisticRegression with TF IDF')\nrun_ml_model(MultinomialNB(), x_train_counts, y_train, x_test_counts, y_test, \n             'MultinomialNB with Bag of words')\nrun_ml_model(MultinomialNB(), x_train_tfidf, y_train, x_test_tfidf, y_test, \n             'MultinomialNB with TF IDF')\nrun_ml_model(SVC(C=1.0, probability=True), x_train_counts, y_train, x_test_counts, y_test, \n             'SVC with Bag of words')\nrun_ml_model(SVC(C=1.0, probability=True), x_train_tfidf, y_train, x_test_tfidf, y_test, \n             'SVC with TF IDF')\nrun_ml_model(RandomForestClassifier(), x_train_counts, y_train, x_test_counts, y_test, \n             'RandomForestClassifier with Bag of words')\nrun_ml_model(RandomForestClassifier(), x_train_tfidf, y_train, x_test_tfidf, y_test, \n             'RandomForestClassifier with TF IDF')","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:28.668978Z","iopub.execute_input":"2023-02-25T17:48:28.669725Z","iopub.status.idle":"2023-02-25T17:48:32.556151Z","shell.execute_reply.started":"2023-02-25T17:48:28.669689Z","shell.execute_reply":"2023-02-25T17:48:32.555060Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"LogisticRegression with Bag of words accuracy - 0.8780260707635009\nLogisticRegression with TF IDF accuracy - 0.8696461824953445\nMultinomialNB with Bag of words accuracy - 0.8770949720670391\nMultinomialNB with TF IDF accuracy - 0.8659217877094972\nSVC with Bag of words accuracy - 0.8864059590316573\nSVC with TF IDF accuracy - 0.88268156424581\nRandomForestClassifier with Bag of words accuracy - 0.8594040968342644\nRandomForestClassifier with TF IDF accuracy - 0.8361266294227188\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Deep Learning Models","metadata":{}},{"cell_type":"markdown","source":"## Architecture 1\nCan be used by datasets with input as tokenized sentences","metadata":{}},{"cell_type":"code","source":"class BiLSTM1(nn.Module):\n    def __init__(self, num_words, embed_size, hidden_size, output_size, dropout_rate):\n        super(BiLSTM1, self).__init__()\n        self.embedding = nn.Embedding(num_words, embed_size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.bilstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        x, _ = self.bilstm1(x)\n        x, _ = self.bilstm2(x)\n        x = self.fc(x[:, -1, :])\n        x = self.sigmoid(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:32.557790Z","iopub.execute_input":"2023-02-25T17:48:32.558185Z","iopub.status.idle":"2023-02-25T17:48:32.566141Z","shell.execute_reply.started":"2023-02-25T17:48:32.558146Z","shell.execute_reply":"2023-02-25T17:48:32.565039Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Architecture 2\nCan be used by tf-idf or bad-of-words representations","metadata":{}},{"cell_type":"code","source":"class BiLSTM2(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n        super(BiLSTM2, self).__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.bilstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x, _ = self.bilstm1(x)\n        x = self.dropout(x)\n        x, _ = self.bilstm2(x)\n        x = x[:, :].unsqueeze(1)\n        x = self.fc(x[:, -1, :])\n        x = self.sigmoid(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:32.567655Z","iopub.execute_input":"2023-02-25T17:48:32.568339Z","iopub.status.idle":"2023-02-25T17:48:32.584144Z","shell.execute_reply.started":"2023-02-25T17:48:32.568303Z","shell.execute_reply":"2023-02-25T17:48:32.583226Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def train_test(model, criterion, optimizer, epochs, x_train, y_train, x_test, y_test, convert_to='long'):\n    y_train = y_train.float()\n    y_test = y_test.float()\n    if convert_to == 'long':\n        x_train = x_train.long()\n        x_test = x_test.long()\n    elif convert_to == 'float':\n        x_train = x_train.float()\n        x_test = x_test.float()\n    model.train()\n    train_accuracy = 0\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        outputs = model(x_train)\n        loss = criterion(outputs.squeeze().to(device), y_train.to(device))\n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            predictions = model(x_train.to(device))\n            predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n        _train_accuracy = metrics.accuracy_score(y_train.to('cpu'), predictions)\n        train_accuracy += _train_accuracy\n\n    train_accuracy /= epochs\n    # Evaluate the model\n    model.eval()\n    with torch.no_grad():\n        predictions = model(x_test.to(device))\n        predictions = (predictions > 0.5).to('cpu').int().squeeze().numpy()\n\n    test_accuracy = metrics.accuracy_score(y_test.to('cpu'), predictions)\n    print(f\"Total Epochs: {epochs}, Train Accuracy: {train_accuracy} Test Accuracy: {test_accuracy}\")\n    return train_accuracy, test_accuracy","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:32.585579Z","iopub.execute_input":"2023-02-25T17:48:32.586087Z","iopub.status.idle":"2023-02-25T17:48:32.596730Z","shell.execute_reply.started":"2023-02-25T17:48:32.586052Z","shell.execute_reply":"2023-02-25T17:48:32.595690Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"set_torch_seed()\ndevice = get_device()","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:32.598252Z","iopub.execute_input":"2023-02-25T17:48:32.598926Z","iopub.status.idle":"2023-02-25T17:48:32.671018Z","shell.execute_reply.started":"2023-02-25T17:48:32.598890Z","shell.execute_reply":"2023-02-25T17:48:32.670337Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"x_train_tk, y_train_tk, x_test_tk, y_test_tk = tokenize_and_generate_sequences(x_train, y_train, x_test, y_test)\nx_train_tk.shape, y_train_tk.shape, x_test_tk.shape, y_test_tk.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:48:32.672275Z","iopub.execute_input":"2023-02-25T17:48:32.672623Z","iopub.status.idle":"2023-02-25T17:48:37.056174Z","shell.execute_reply.started":"2023-02-25T17:48:32.672577Z","shell.execute_reply":"2023-02-25T17:48:37.055056Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1609, 60]),\n torch.Size([1609]),\n torch.Size([1074, 60]),\n torch.Size([1074]))"},"metadata":{}}]},{"cell_type":"code","source":"te = 25\nfor e in range(1, te+1):\n    model = BiLSTM1(num_words=1000, embed_size=60, hidden_size=64, output_size=1, dropout_rate=0.2).to(device)\n    criterion = nn.BCELoss().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    train_test(model, criterion, optimizer, e, x_train_tk, y_train_tk, x_test_tk, y_test_tk)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:49:36.636552Z","iopub.execute_input":"2023-02-25T17:49:36.637334Z","iopub.status.idle":"2023-02-25T17:49:55.467845Z","shell.execute_reply.started":"2023-02-25T17:49:36.637297Z","shell.execute_reply":"2023-02-25T17:49:55.466724Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Total Epochs: 1, Train Accuracy: 0.6432566811684276 Test Accuracy: 0.6787709497206704\nTotal Epochs: 2, Train Accuracy: 0.6190180236171535 Test Accuracy: 0.6024208566108007\nTotal Epochs: 3, Train Accuracy: 0.6668738346799254 Test Accuracy: 0.6824953445065177\nTotal Epochs: 4, Train Accuracy: 0.6884711000621504 Test Accuracy: 0.7318435754189944\nTotal Epochs: 5, Train Accuracy: 0.6899937849596023 Test Accuracy: 0.7150837988826816\nTotal Epochs: 6, Train Accuracy: 0.6935985083903046 Test Accuracy: 0.7150837988826816\nTotal Epochs: 7, Train Accuracy: 0.6969723874633756 Test Accuracy: 0.7579143389199255\nTotal Epochs: 8, Train Accuracy: 0.7277812305779987 Test Accuracy: 0.7364990689013036\nTotal Epochs: 9, Train Accuracy: 0.7159035978178303 Test Accuracy: 0.74487895716946\nTotal Epochs: 10, Train Accuracy: 0.7625233064014916 Test Accuracy: 0.7905027932960894\nTotal Epochs: 11, Train Accuracy: 0.7562574156732018 Test Accuracy: 0.792364990689013\nTotal Epochs: 12, Train Accuracy: 0.7737207375181271 Test Accuracy: 0.8165735567970205\nTotal Epochs: 13, Train Accuracy: 0.7646412009370369 Test Accuracy: 0.8035381750465549\nTotal Epochs: 14, Train Accuracy: 0.7904199591583059 Test Accuracy: 0.8314711359404097\nTotal Epochs: 15, Train Accuracy: 0.8103584006629375 Test Accuracy: 0.8333333333333334\nTotal Epochs: 16, Train Accuracy: 0.8227548166563082 Test Accuracy: 0.8258845437616388\nTotal Epochs: 17, Train Accuracy: 0.7570284795086462 Test Accuracy: 0.8044692737430168\nTotal Epochs: 18, Train Accuracy: 0.8111663559146467 Test Accuracy: 0.835195530726257\nTotal Epochs: 19, Train Accuracy: 0.8099178960452718 Test Accuracy: 0.8296089385474861\nTotal Epochs: 20, Train Accuracy: 0.8200745804847731 Test Accuracy: 0.8230912476722533\nTotal Epochs: 21, Train Accuracy: 0.8490337091953002 Test Accuracy: 0.8500931098696461\nTotal Epochs: 22, Train Accuracy: 0.8559240635064128 Test Accuracy: 0.845437616387337\nTotal Epochs: 23, Train Accuracy: 0.841786689004783 Test Accuracy: 0.845437616387337\nTotal Epochs: 24, Train Accuracy: 0.8613010151232651 Test Accuracy: 0.8445065176908753\nTotal Epochs: 25, Train Accuracy: 0.8653325046612804 Test Accuracy: 0.8687150837988827\n","output_type":"stream"}]},{"cell_type":"code","source":"x = pd.concat([x_train, x_test])\nx = x.apply(lambda xx: stemm_text(xx))\nsplit_len = x_train.shape[0]\nx = [x[i:i+split_len] for i in range(0, len(x), split_len)]\nx_train_stemmed = x[0]\nx_test_stemmed = x[1]\nx_train_stemmed.shape, x_test_stemmed.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:49:55.470704Z","iopub.execute_input":"2023-02-25T17:49:55.471037Z","iopub.status.idle":"2023-02-25T17:49:56.132569Z","shell.execute_reply.started":"2023-02-25T17:49:55.471007Z","shell.execute_reply":"2023-02-25T17:49:56.131608Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"((1609,), (1074,))"},"metadata":{}}]},{"cell_type":"code","source":"x_train_stemmed_tk, y_train_tk, x_test_stemmed_tk, y_test_tk = tokenize_and_generate_sequences(x_train_stemmed, y_train, x_test_stemmed, y_test)\nx_train_stemmed_tk.shape, y_train_tk.shape, x_test_stemmed_tk.shape, y_test_tk.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:49:56.134027Z","iopub.execute_input":"2023-02-25T17:49:56.134405Z","iopub.status.idle":"2023-02-25T17:49:56.212834Z","shell.execute_reply.started":"2023-02-25T17:49:56.134360Z","shell.execute_reply":"2023-02-25T17:49:56.211793Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1609, 60]),\n torch.Size([1609]),\n torch.Size([1074, 60]),\n torch.Size([1074]))"},"metadata":{}}]},{"cell_type":"code","source":"te = 25\nfor e in range(1, te+1):\n    model = BiLSTM1(num_words=1000, embed_size=60, hidden_size=64, output_size=1, dropout_rate=0.2).to(device)\n    criterion = nn.BCELoss().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    train_test(model, criterion, optimizer, e, x_train_stemmed_tk, y_train_tk, x_test_stemmed_tk, y_test_tk)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:49:56.215248Z","iopub.execute_input":"2023-02-25T17:49:56.215672Z","iopub.status.idle":"2023-02-25T17:50:14.946475Z","shell.execute_reply.started":"2023-02-25T17:49:56.215636Z","shell.execute_reply":"2023-02-25T17:50:14.945177Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Total Epochs: 1, Train Accuracy: 0.6687383467992541 Test Accuracy: 0.6880819366852886\nTotal Epochs: 2, Train Accuracy: 0.6733996270975762 Test Accuracy: 0.6312849162011173\nTotal Epochs: 3, Train Accuracy: 0.6728817070644292 Test Accuracy: 0.7104283054003724\nTotal Epochs: 4, Train Accuracy: 0.6452765692977005 Test Accuracy: 0.6266294227188082\nTotal Epochs: 5, Train Accuracy: 0.7006836544437538 Test Accuracy: 0.7383612662942272\nTotal Epochs: 6, Train Accuracy: 0.725088046405635 Test Accuracy: 0.7216014897579144\nTotal Epochs: 7, Train Accuracy: 0.7534404687916185 Test Accuracy: 0.7597765363128491\nTotal Epochs: 8, Train Accuracy: 0.7417650714729646 Test Accuracy: 0.7588454376163873\nTotal Epochs: 9, Train Accuracy: 0.735377391064153 Test Accuracy: 0.7420856610800745\nTotal Epochs: 10, Train Accuracy: 0.7438781852082039 Test Accuracy: 0.7625698324022346\nTotal Epochs: 11, Train Accuracy: 0.7506073789479631 Test Accuracy: 0.7802607076350093\nTotal Epochs: 12, Train Accuracy: 0.7544541122850633 Test Accuracy: 0.7905027932960894\nTotal Epochs: 13, Train Accuracy: 0.77578046564995 Test Accuracy: 0.7877094972067039\nTotal Epochs: 14, Train Accuracy: 0.8007635621060107 Test Accuracy: 0.8333333333333334\nTotal Epochs: 15, Train Accuracy: 0.7840480629790759 Test Accuracy: 0.7942271880819367\nTotal Epochs: 16, Train Accuracy: 0.8131991920447483 Test Accuracy: 0.8342644320297952\nTotal Epochs: 17, Train Accuracy: 0.8311702555478375 Test Accuracy: 0.8044692737430168\nTotal Epochs: 18, Train Accuracy: 0.8410676058283266 Test Accuracy: 0.8566108007448789\nTotal Epochs: 19, Train Accuracy: 0.836544437538844 Test Accuracy: 0.8594040968342644\nTotal Epochs: 20, Train Accuracy: 0.819577377252952 Test Accuracy: 0.813780260707635\nTotal Epochs: 21, Train Accuracy: 0.8300926336973573 Test Accuracy: 0.8426443202979516\nTotal Epochs: 22, Train Accuracy: 0.8689191479744619 Test Accuracy: 0.8538175046554934\nTotal Epochs: 23, Train Accuracy: 0.8654038425162807 Test Accuracy: 0.8696461824953445\nTotal Epochs: 24, Train Accuracy: 0.8726693598508392 Test Accuracy: 0.8575418994413407\nTotal Epochs: 25, Train Accuracy: 0.8763952765692976 Test Accuracy: 0.8538175046554934\n","output_type":"stream"}]},{"cell_type":"code","source":"# nltk.download('wordnet')\nx = pd.concat([x_train, x_test])\nx.reset_index(drop=True, inplace=True)\nx = x.apply(lambda yy: lemmatize_text(yy))\nsplit_len = x_train.shape[0]\nx = [x[i:i+split_len] for i in range(0, len(x), split_len)]\nx_train_lemmatize = x[0]\nx_test_lemmatize = x[1]\nx_train_lemmatize.shape, x_test_lemmatize.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:50:14.948014Z","iopub.execute_input":"2023-02-25T17:50:14.948402Z","iopub.status.idle":"2023-02-25T17:50:16.577171Z","shell.execute_reply.started":"2023-02-25T17:50:14.948364Z","shell.execute_reply":"2023-02-25T17:50:16.576057Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"((1609,), (1074,))"},"metadata":{}}]},{"cell_type":"code","source":"x_train_lemmatize_tk, y_train_tk, x_test_lemmatize_tk, y_test_tk = tokenize_and_generate_sequences(x_train_lemmatize, y_train, x_test_lemmatize, y_test)\nx_train_lemmatize_tk.shape, y_train_tk.shape, x_test_lemmatize_tk.shape, y_test_tk.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:50:16.578532Z","iopub.execute_input":"2023-02-25T17:50:16.579185Z","iopub.status.idle":"2023-02-25T17:50:16.658205Z","shell.execute_reply.started":"2023-02-25T17:50:16.579143Z","shell.execute_reply":"2023-02-25T17:50:16.657088Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1609, 60]),\n torch.Size([1609]),\n torch.Size([1074, 60]),\n torch.Size([1074]))"},"metadata":{}}]},{"cell_type":"code","source":"te = 50\nfor e in range(1, te+1):\n    model = BiLSTM1(num_words=1000, embed_size=60, hidden_size=64, output_size=1, dropout_rate=0.2).to(device)\n    criterion = nn.BCELoss().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    train_test(model, criterion, optimizer, e, x_train_lemmatize_tk, y_train_tk, x_test_lemmatize_tk, y_test_tk, 'long')","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:50:16.659590Z","iopub.execute_input":"2023-02-25T17:50:16.660233Z","iopub.status.idle":"2023-02-25T17:51:29.643126Z","shell.execute_reply.started":"2023-02-25T17:50:16.660189Z","shell.execute_reply":"2023-02-25T17:51:29.642024Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Total Epochs: 1, Train Accuracy: 0.6556867619639528 Test Accuracy: 0.6834264432029795\nTotal Epochs: 2, Train Accuracy: 0.6793039154754505 Test Accuracy: 0.6517690875232774\nTotal Epochs: 3, Train Accuracy: 0.62958359229335 Test Accuracy: 0.6722532588454376\nTotal Epochs: 4, Train Accuracy: 0.699658172778123 Test Accuracy: 0.6973929236499069\nTotal Epochs: 5, Train Accuracy: 0.6944686140459913 Test Accuracy: 0.7188081936685289\nTotal Epochs: 6, Train Accuracy: 0.722084110213383 Test Accuracy: 0.7337057728119181\nTotal Epochs: 7, Train Accuracy: 0.7164165852792329 Test Accuracy: 0.7607076350093109\nTotal Epochs: 8, Train Accuracy: 0.7160503418272218 Test Accuracy: 0.7700186219739292\nTotal Epochs: 9, Train Accuracy: 0.7400041433602652 Test Accuracy: 0.7625698324022346\nTotal Epochs: 10, Train Accuracy: 0.7456183965195774 Test Accuracy: 0.7830540037243948\nTotal Epochs: 11, Train Accuracy: 0.7797615684501948 Test Accuracy: 0.813780260707635\nTotal Epochs: 12, Train Accuracy: 0.7784856018230785 Test Accuracy: 0.8054003724394786\nTotal Epochs: 13, Train Accuracy: 0.7983458430941341 Test Accuracy: 0.8361266294227188\nTotal Epochs: 14, Train Accuracy: 0.7928171890260144 Test Accuracy: 0.8491620111731844\nTotal Epochs: 15, Train Accuracy: 0.8073751812720116 Test Accuracy: 0.8435754189944135\nTotal Epochs: 16, Train Accuracy: 0.8059353635798632 Test Accuracy: 0.8277467411545624\nTotal Epochs: 17, Train Accuracy: 0.8261616641684641 Test Accuracy: 0.8342644320297952\nTotal Epochs: 18, Train Accuracy: 0.8398936537531939 Test Accuracy: 0.8659217877094972\nTotal Epochs: 19, Train Accuracy: 0.8255209185175493 Test Accuracy: 0.8547486033519553\nTotal Epochs: 20, Train Accuracy: 0.8354878806712243 Test Accuracy: 0.8286778398510242\nTotal Epochs: 21, Train Accuracy: 0.8521412293941817 Test Accuracy: 0.8491620111731844\nTotal Epochs: 22, Train Accuracy: 0.8488050172326118 Test Accuracy: 0.845437616387337\nTotal Epochs: 23, Train Accuracy: 0.8577296187207826 Test Accuracy: 0.8696461824953445\nTotal Epochs: 24, Train Accuracy: 0.8602133830536564 Test Accuracy: 0.8687150837988827\nTotal Epochs: 25, Train Accuracy: 0.8756743318831575 Test Accuracy: 0.8743016759776536\nTotal Epochs: 26, Train Accuracy: 0.8780178801931444 Test Accuracy: 0.8575418994413407\nTotal Epochs: 27, Train Accuracy: 0.8884515341942317 Test Accuracy: 0.8659217877094972\nTotal Epochs: 28, Train Accuracy: 0.8822027878895503 Test Accuracy: 0.8640595903165735\nTotal Epochs: 29, Train Accuracy: 0.8701270868605474 Test Accuracy: 0.8566108007448789\nTotal Epochs: 30, Train Accuracy: 0.8925833851253366 Test Accuracy: 0.8491620111731844\nTotal Epochs: 31, Train Accuracy: 0.8954870787305281 Test Accuracy: 0.8547486033519553\nTotal Epochs: 32, Train Accuracy: 0.9017635177128652 Test Accuracy: 0.8621973929236499\nTotal Epochs: 33, Train Accuracy: 0.8924986345744581 Test Accuracy: 0.8733705772811918\nTotal Epochs: 34, Train Accuracy: 0.8901034621430924 Test Accuracy: 0.8687150837988827\nTotal Epochs: 35, Train Accuracy: 0.9020864778478203 Test Accuracy: 0.87243947858473\nTotal Epochs: 36, Train Accuracy: 0.9027864097783301 Test Accuracy: 0.8659217877094972\nTotal Epochs: 37, Train Accuracy: 0.9050442611660762 Test Accuracy: 0.8715083798882681\nTotal Epochs: 38, Train Accuracy: 0.8886362892937753 Test Accuracy: 0.8575418994413407\nTotal Epochs: 39, Train Accuracy: 0.9162722506414238 Test Accuracy: 0.8575418994413407\nTotal Epochs: 40, Train Accuracy: 0.9174331883157241 Test Accuracy: 0.8677839851024208\nTotal Epochs: 41, Train Accuracy: 0.9196743925176977 Test Accuracy: 0.866852886405959\nTotal Epochs: 42, Train Accuracy: 0.9230370830743734 Test Accuracy: 0.8743016759776536\nTotal Epochs: 43, Train Accuracy: 0.9307528871030687 Test Accuracy: 0.8556797020484171\nTotal Epochs: 44, Train Accuracy: 0.9117605514435844 Test Accuracy: 0.8519553072625698\nTotal Epochs: 45, Train Accuracy: 0.9191492300255509 Test Accuracy: 0.8649906890130353\nTotal Epochs: 46, Train Accuracy: 0.9310941173291539 Test Accuracy: 0.8547486033519553\nTotal Epochs: 47, Train Accuracy: 0.9338296549991404 Test Accuracy: 0.8733705772811918\nTotal Epochs: 48, Train Accuracy: 0.9318287756370416 Test Accuracy: 0.8733705772811918\nTotal Epochs: 49, Train Accuracy: 0.9340444692482337 Test Accuracy: 0.8556797020484171\nTotal Epochs: 50, Train Accuracy: 0.9374891236793041 Test Accuracy: 0.8696461824953445\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Architecture 3\n### SetFitClassifier transfer learning and few shot prediction","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport random\nfrom collections import defaultdict\nfrom itertools import chain, groupby\nfrom typing import Any, List, Optional, Union\n\nimport joblib\nimport numpy as np\nimport torch\nfrom sentence_transformers import InputExample, SentenceTransformer, losses\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.linear_model import LogisticRegression\nfrom torch.utils.data import DataLoader\n\nStrOrPath = Union[Path, str]\n\n\ndef generate_sentence_pair_batch(\n    sentences: List[str], labels: List[float]\n) -> List[InputExample]:\n    # 7x faster than original implementation on small data,\n    # 14x faster on 10000 examples\n    pairs = []\n    sent_lookup = defaultdict(list)\n    single_example = {}\n    for label, grouper in groupby(\n        ((s, l) for s, l in zip(sentences, labels)), key=lambda x: x[1]\n    ):\n        sent_lookup[label].extend(list(i[0] for i in grouper))\n        single_example[label] = len(sent_lookup[label]) == 1\n    neg_lookup = {}\n    for current_label in sent_lookup:\n        negative_options = list(\n            chain.from_iterable(\n                [\n                    sentences\n                    for label, sentences in sent_lookup.items()\n                    if label != current_label\n                ]\n            )\n        )\n        neg_lookup[current_label] = negative_options\n\n    for current_sentence, current_label in zip(sentences, labels):\n        positive_pair = random.choice(sent_lookup[current_label])\n        if not single_example[current_label]:\n            # choosing itself as a matched pair seems wrong,\n            # but we need to account for the case of 1 positive example\n            # so as long as there's not a single positive example,\n            # we'll reselect the other item in the pair until it's different\n            while positive_pair == current_sentence:\n                positive_pair = random.choice(sent_lookup[current_label])\n\n        negative_pair = random.choice(neg_lookup[current_label])\n        pairs.append(InputExample(texts=[current_sentence, positive_pair], label=1.0))\n        pairs.append(InputExample(texts=[current_sentence, negative_pair], label=0.0))\n\n    return pairs\n\n\ndef generate_multiple_sentence_pairs(\n    sentences: List[str], labels: List[float], iter: int = 1\n):\n    all_pairs = []\n    for _ in range(iter):\n        all_pairs.extend(generate_sentence_pair_batch(sentences, labels))\n    return all_pairs\n\n\nclass SetFitClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(\n        self,\n        model: str,\n        classifier_head: Optional[Any] = None,\n        loss=losses.CosineSimilarityLoss,\n        random_state: int = 1234,\n    ):\n        random.seed(random_state)\n        np.random.seed(random_state)\n        torch.manual_seed(random_state)\n        self.random_state = random_state\n        self.model = SentenceTransformer(model)\n        if classifier_head is None:\n            self.classifier_head = LogisticRegression()\n        else:\n            self.classifier_head = classifier_head()\n        self.loss = loss(self.model)\n        self.fitted = False\n\n    def fit(\n        self,\n        X,\n        y,\n        data_iter: int = 5,\n        train_iter: int = 1,\n        batch_size: int = 16,\n        warmup_steps: int = 10,\n        show_progress_bar: bool = True,\n    ):\n        train_examples = generate_multiple_sentence_pairs(X, y, data_iter)\n        train_dataloader = DataLoader(\n            train_examples,\n            shuffle=True,\n            batch_size=batch_size,\n            generator=torch.Generator(device=self.model.device),\n        )\n        self.model.fit(\n            train_objectives=[(train_dataloader, self.loss)],\n            epochs=train_iter,\n            warmup_steps=warmup_steps,\n            show_progress_bar=show_progress_bar,\n        )\n\n        X_train = self.model.encode(X)\n        self.classifier_head.fit(X_train, y)\n        self.fitted = True\n\n    def predict(self, X, y=None):\n        if not self.fitted:\n            raise NotFittedError(\n                \"This SetFitClassifier instance is not fitted yet.\"\n                \" Call 'fit' with appropriate arguments before using this estimator.\"\n            )\n        X_embed = self.model.encode(X)\n        preds = self.classifier_head.predict(X_embed)\n        return preds\n\n    def predict_proba(self, X, y=None):\n        if not self.fitted:\n            raise NotFittedError(\n                \"This SetFitClassifier instance is not fitted yet.\"\n                \" Call 'fit' with appropriate arguments before using this estimator.\"\n            )\n        X_embed = self.model.encode(X)\n        preds = self.classifier_head.predict_proba(X_embed)\n        return preds\n\n    def save(\n        self,\n        path: StrOrPath,\n        model_name: Optional[str] = None,\n        create_model_card: bool = False,\n    ):\n        if not self.fitted:\n            raise NotFittedError(\n                \"This SetFitClassifier instance is not fitted yet.\"\n                \" Call 'fit' with appropriate arguments before saving this estimator.\"\n            )\n        self.model.save(str(path), model_name, create_model_card)\n        joblib.dump(self.classifier_head, Path(path) / \"classifier.pkl\")\n\n    @classmethod\n    def load(cls, path: StrOrPath):\n        setfit = SetFitClassifier(str(path))\n        setfit.classifier_head = joblib.load(Path(path) / \"classifier.pkl\")\n        setfit.fitted = True\n        return setfit","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-02-25T17:49:24.225948Z","iopub.status.idle":"2023-02-25T17:49:24.226624Z","shell.execute_reply.started":"2023-02-25T17:49:24.226357Z","shell.execute_reply":"2023-02-25T17:49:24.226382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}